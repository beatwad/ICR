{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:23.665663Z",
     "iopub.status.busy": "2023-06-08T15:31:23.665223Z",
     "iopub.status.idle": "2023-06-08T15:31:35.997437Z",
     "shell.execute_reply": "2023-06-08T15:31:35.996163Z",
     "shell.execute_reply.started": "2023-06-08T15:31:23.665630Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, BaseShuffleSplit, _validate_shuffle_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.validation import _num_samples, check_array\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "import eli5\n",
    "from IPython.display import display\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:36.001085Z",
     "iopub.status.busy": "2023-06-08T15:31:35.999808Z",
     "iopub.status.idle": "2023-06-08T15:31:36.066222Z",
     "shell.execute_reply": "2023-06-08T15:31:36.064862Z",
     "shell.execute_reply.started": "2023-06-08T15:31:36.001050Z"
    }
   },
   "outputs": [],
   "source": [
    "# COMP_PATH = \"/kaggle/input/icr-identify-age-related-conditions\"\n",
    "COMP_PATH = \"icr-identify-age-related-conditions\"\n",
    "\n",
    "train_df = pd.read_csv(f'{COMP_PATH}//train.csv')\n",
    "test_df = pd.read_csv(f'{COMP_PATH}/test.csv')\n",
    "greeks = pd.read_csv(f\"{COMP_PATH}/greeks.csv\")\n",
    "sample_submission = pd.read_csv(f\"{COMP_PATH}/sample_submission.csv\")\n",
    "\n",
    "train_df['EJ'] = train_df['EJ'].replace({'A': 0, 'B': 1})\n",
    "test_df['EJ'] = test_df['EJ'].replace({'A': 0, 'B': 1})\n",
    "\n",
    "train_df.columns = train_df.columns.str.replace(' ', '')\n",
    "test_df.columns = test_df.columns.str.replace(' ', '')\n",
    "\n",
    "# train_df.drop('Id',axis=1, inplace=True)\n",
    "# train_df.fillna(train_df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:36.069293Z",
     "iopub.status.busy": "2023-06-08T15:31:36.068506Z",
     "iopub.status.idle": "2023-06-08T15:31:36.075275Z",
     "shell.execute_reply": "2023-06-08T15:31:36.074052Z",
     "shell.execute_reply.started": "2023-06-08T15:31:36.069253Z"
    }
   },
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# new_num_cols = train_df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# train_df[new_num_cols] = scaler.fit_transform(train_df[new_num_cols])\n",
    "# test_df[new_num_cols] = scaler.transform(test_df[new_num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine features in all possible ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi = pd.read_csv('feature_importances.csv', index_col = 'Unnamed: 0')\n",
    "# fi_cols = set(fi['Feature'].head(100).values)\n",
    "\n",
    "# perm = pd.read_csv('perm_df.csv', index_col = 'Unnamed: 0')\n",
    "# perm_cols = set(perm['importance'].head(100).index)\n",
    "\n",
    "# important_col = list(perm_cols.intersection(fi_cols))\n",
    "# print(important_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [fe for fe in train_df.columns if fe not in ['Id','CF', 'CB', 'DV', 'BR', 'DF', 'AR', 'GI', 'BD ','AY','AM','GB',\n",
    "#                                                         'AH', 'CW ', 'FS', 'GE', 'CL', 'AX', 'Class', 'BP']]\n",
    "\n",
    "# for f in features:\n",
    "#     train_df[f] = np.floor(train_df[f]*1000)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:36.080972Z",
     "iopub.status.busy": "2023-06-08T15:31:36.079281Z",
     "iopub.status.idle": "2023-06-08T15:31:44.412385Z",
     "shell.execute_reply": "2023-06-08T15:31:44.411508Z",
     "shell.execute_reply.started": "2023-06-08T15:31:36.080912Z"
    }
   },
   "outputs": [],
   "source": [
    "# features = train_df.drop(['Class', 'Id'], axis=1).columns\n",
    "\n",
    "\n",
    "features = [fe for fe in train_df.columns if fe not in ['Id','CF', 'CB', 'DV', 'BR', 'DF', 'AR', 'GI', 'AY', 'GB',\n",
    "                                                        'AH', 'CW', 'CL', 'Class', 'BP']]\n",
    "\n",
    "# selected = ['CR+DU', 'EH-FI', 'CD-DL', 'AB-FI', 'FL+GL', 'DU-EP', 'BQ+EP', 'FR+GL', 'CD-EP', 'AF-EG', \n",
    "#             'CU-DU', 'AB-CR', 'BQ+CD', 'BQ-GF', 'BQ+DI', 'CR+DH', 'CD+EL', 'CC+EH', 'DA-DU', 'AF+BQ', \n",
    "#             'CR+EH', 'BQ-DN', 'DN-EL', 'EJ+FI', 'BQ-EP', 'EE*EP', 'BQ/CU', 'AB*FR', 'EJ/FL', 'CD/CH', \n",
    "#             'CD/EP', 'CR*DU', 'AF/EG', 'CC/CD', 'AB/CH', 'BQ/CH', 'CH/DI', 'AB/DN', 'DU/EP', 'DU/EJ', \n",
    "#             'DU/GL', 'DY/EE', 'CR*CS', 'DH/DU', 'EJ*GL', 'CD/DL', 'DH/DI', 'DU*FR', 'CR*DH']\n",
    "\n",
    "# def gen_features(features, df):\n",
    "#     generated_features = pd.DataFrame()\n",
    "\n",
    "#     for fe_a, fe_b in tqdm(itertools.combinations(features, 2), total=sum([1 for i in itertools.combinations(features, 2)])):\n",
    "#         generated_features[f'{fe_a}+{fe_b}']   = df[fe_a] + df[fe_b]\n",
    "#         generated_features[f'{fe_a}-{fe_b}']   = df[fe_a] - df[fe_b] \n",
    "#         generated_features[f'{fe_a}*{fe_b}']   = df[fe_a] * df[fe_b]\n",
    "#         generated_features[f'{fe_a}/{fe_b}']   = df[fe_a] / df[fe_b]\n",
    "\n",
    "# #         generated_features[f'{fe_a}_2']        = df[fe_a].pow(2)\n",
    "# #         generated_features[f'{fe_b}_2']        = df[fe_b].pow(2)\n",
    "# #         generated_features[f'{fe_a}*{fe_b}_2'] = df[fe_a] * df[fe_b].pow(2)\n",
    "# #         generated_features[f'{fe_a}_2*{fe_b}'] = df[fe_a].pow(2) * df[fe_b]\n",
    "\n",
    "# #         generated_features[f'{fe_a}_05'] = df[fe_a].pow(0.5)\n",
    "# #         generated_features[f'{fe_b}_05'] = df[fe_b].pow(0.5)\n",
    "# #         generated_features[f'{fe_a}*{fe_b}_05'] = df[fe_a] * df[fe_b].pow(0.5)\n",
    "# #         generated_features[f'{fe_a}_05*{fe_b}'] = df[fe_a].pow(0.5) * df[fe_b]\n",
    "\n",
    "# #         generated_features[f'{fe_a}_log'] = np.log(df[fe_a])\n",
    "# #         generated_features[f'{fe_b}_log'] = np.log(df[fe_b])\n",
    "# #         generated_features[f'{fe_a}*{fe_b}_log'] = df[fe_a] * np.log(df[fe_b])\n",
    "# #         generated_features[f'{fe_a}_log*{fe_b}'] = np.log(df[fe_a]) * df[fe_b]\n",
    "        \n",
    "#     generated_features = generated_features[selected]\n",
    "#     generated_features = pd.concat([generated_features, df[features]], axis=1)\n",
    "    \n",
    "#     # prevent inf\n",
    "#     for g in generated_features.columns:\n",
    "#         generated_features[g] = np.minimum(np.maximum(generated_features[g], -1e9), 1e9)\n",
    "    \n",
    "#     return generated_features\n",
    "\n",
    "# generated_features_train = gen_features(features, train_df)\n",
    "# generated_features_test = gen_features(features, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:44.414098Z",
     "iopub.status.busy": "2023-06-08T15:31:44.413784Z",
     "iopub.status.idle": "2023-06-08T15:31:44.461531Z",
     "shell.execute_reply": "2023-06-08T15:31:44.460184Z",
     "shell.execute_reply.started": "2023-06-08T15:31:44.414071Z"
    }
   },
   "outputs": [],
   "source": [
    "def IterativeStratification(labels, r, random_state):\n",
    "    \"\"\"This function implements the Iterative Stratification algorithm described\n",
    "    in the following paper:\n",
    "    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n",
    "    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n",
    "    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n",
    "    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n",
    "    Heidelberg.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = labels.shape[0]\n",
    "    test_folds = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    # Calculate the desired number of examples at each subset\n",
    "    c_folds = r * n_samples\n",
    "\n",
    "    # Calculate the desired number of examples of each label at each subset\n",
    "    c_folds_labels = np.outer(r, labels.sum(axis=0))\n",
    "\n",
    "    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n",
    "\n",
    "    while np.any(labels_not_processed_mask):\n",
    "        # Find the label with the fewest (but at least one) remaining examples,\n",
    "        # breaking ties randomly\n",
    "        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n",
    "\n",
    "        # Handle case where only all-zero labels are left by distributing\n",
    "        # across all folds as evenly as possible (not in original algorithm but\n",
    "        # mentioned in the text). (By handling this case separately, some\n",
    "        # code redundancy is introduced; however, this approach allows for\n",
    "        # decreased execution time when there are a relatively large number\n",
    "        # of all-zero labels.)\n",
    "        if num_labels.sum() == 0:\n",
    "            sample_idxs = np.where(labels_not_processed_mask)[0]\n",
    "\n",
    "            for sample_idx in sample_idxs:\n",
    "                fold_idx = np.where(c_folds == c_folds.max())[0]\n",
    "\n",
    "                if fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n",
    "\n",
    "                test_folds[sample_idx] = fold_idx\n",
    "                c_folds[fold_idx] -= 1\n",
    "\n",
    "            break\n",
    "\n",
    "        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n",
    "        if label_idx.shape[0] > 1:\n",
    "            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n",
    "\n",
    "        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n",
    "\n",
    "        for sample_idx in sample_idxs:\n",
    "            # Find the subset(s) with the largest number of desired examples\n",
    "            # for this label, breaking ties by considering the largest number\n",
    "            # of desired examples, breaking further ties randomly\n",
    "            label_folds = c_folds_labels[:, label_idx]\n",
    "            fold_idx = np.where(label_folds == label_folds.max())[0]\n",
    "\n",
    "            if fold_idx.shape[0] > 1:\n",
    "                temp_fold_idx = np.where(c_folds[fold_idx] ==\n",
    "                                         c_folds[fold_idx].max())[0]\n",
    "                fold_idx = fold_idx[temp_fold_idx]\n",
    "\n",
    "                if temp_fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n",
    "\n",
    "            test_folds[sample_idx] = fold_idx\n",
    "            labels_not_processed_mask[sample_idx] = False\n",
    "\n",
    "            # Update desired number of examples\n",
    "            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n",
    "            c_folds[fold_idx] -= 1\n",
    "\n",
    "    return test_folds\n",
    "\n",
    "\n",
    "class MultilabelStratifiedKFold(_BaseKFold):\n",
    "    \"\"\"Multilabel stratified K-Folds cross-validator\n",
    "    Provides train/test indices to split multilabel data into train/test sets.\n",
    "    This cross-validation object is a variation of KFold that returns\n",
    "    stratified folds for multilabel data. The folds are made by preserving\n",
    "    the percentage of samples for each label.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=3\n",
    "        Number of folds. Must be at least 2.\n",
    "    shuffle : boolean, optional\n",
    "        Whether to shuffle each stratification of the data before splitting\n",
    "        into batches.\n",
    "    random_state : int, RandomState instance or None, optional, default=None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedKFold that only uses random_state\n",
    "        when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n",
    "    >>> mskf.get_n_splits(X, y)\n",
    "    2\n",
    "    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n",
    "    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n",
    "    >>> for train_index, test_index in mskf.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different in each fold.\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=3, *, shuffle=False, random_state=None):\n",
    "        super(MultilabelStratifiedKFold, self).__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def _make_test_folds(self, X, y):\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n",
    "\n",
    "        num_samples = y.shape[0]\n",
    "\n",
    "        rng = check_random_state(self.random_state)\n",
    "        indices = np.arange(num_samples)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(indices)\n",
    "            y = y[indices]\n",
    "\n",
    "        r = np.asarray([1 / self.n_splits] * self.n_splits)\n",
    "\n",
    "        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "        return test_folds[np.argsort(indices)]\n",
    "\n",
    "    def _iter_test_masks(self, X=None, y=None, groups=None):\n",
    "        test_folds = self._make_test_folds(X, y)\n",
    "        for i in range(self.n_splits):\n",
    "            yield test_folds == i\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n",
    "\n",
    "\n",
    "class RepeatedMultilabelStratifiedKFold(_RepeatedSplits):\n",
    "    \"\"\"Repeated Multilabel Stratified K-Fold cross validator.\n",
    "    Repeats Mulilabel Stratified K-Fold n times with different randomization\n",
    "    in each repetition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of folds. Must be at least 2.\n",
    "    n_repeats : int, default=10\n",
    "        Number of times cross-validator needs to be repeated.\n",
    "    random_state : None, int or RandomState, default=None\n",
    "        Random state to be used to generate random state for each\n",
    "        repetition as well as randomly breaking ties within the iterative\n",
    "        stratification algorithm.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> rmskf = RepeatedMultilabelStratifiedKFold(n_splits=2, n_repeats=2,\n",
    "    ...     random_state=0)\n",
    "    >>> for train_index, test_index in rmskf.split(X, y):\n",
    "    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...     X_train, X_test = X[train_index], X[test_index]\n",
    "    ...     y_train, y_test = y[train_index], y[test_index]\n",
    "    ...\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [0 1 4 5] TEST: [2 3 6 7]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedStratifiedKFold: Repeats (Non-multilabel) Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, *, n_repeats=10, random_state=None):\n",
    "        super(RepeatedMultilabelStratifiedKFold, self).__init__(\n",
    "            MultilabelStratifiedKFold, n_repeats=n_repeats, random_state=random_state,\n",
    "            n_splits=n_splits)\n",
    "\n",
    "\n",
    "class MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n",
    "    \"\"\"Multilabel Stratified ShuffleSplit cross-validator\n",
    "    Provides train/test indices to split data into train/test sets.\n",
    "    This cross-validation object is a merge of MultilabelStratifiedKFold and\n",
    "    ShuffleSplit, which returns stratified randomized folds for multilabel\n",
    "    data. The folds are made by preserving the percentage of each label.\n",
    "    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n",
    "    do not guarantee that all folds will be different, although this is\n",
    "    still very likely for sizeable datasets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default 10\n",
    "        Number of re-shuffling & splitting iterations.\n",
    "    test_size : float, int, None, optional\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split. If int, represents the\n",
    "        absolute number of test samples. If None, the value is set to the\n",
    "        complement of the train size. By default, the value is set to 0.1.\n",
    "        The default will change in version 0.21. It will remain 0.1 only\n",
    "        if ``train_size`` is unspecified, otherwise it will complement\n",
    "        the specified ``train_size``.\n",
    "    train_size : float, int, or None, default is None\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedShuffleSplit that only uses\n",
    "        random_state when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n",
    "    ...    random_state=0)\n",
    "    >>> msss.get_n_splits(X, y)\n",
    "    3\n",
    "    >>> print(mss)       # doctest: +ELLIPSIS\n",
    "    MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n",
    "                                     train_size=None)\n",
    "    >>> for train_index, test_index in msss.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different from desired due to the\n",
    "    preference of stratification over perfectly sized folds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=10, *, test_size=\"default\", train_size=None,\n",
    "                 random_state=None):\n",
    "        super(MultilabelStratifiedShuffleSplit, self).__init__(\n",
    "            n_splits=n_splits, test_size=test_size, train_size=train_size, random_state=random_state)\n",
    "\n",
    "    def _iter_indices(self, X, y, groups=None):\n",
    "        n_samples = _num_samples(X)\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n",
    "                    type_of_target_y))\n",
    "\n",
    "        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n",
    "                                                  self.train_size)\n",
    "\n",
    "        n_samples = y.shape[0]\n",
    "        rng = check_random_state(self.random_state)\n",
    "        y_orig = y.copy()\n",
    "\n",
    "        r = np.array([n_train, n_test]) / (n_train + n_test)\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            indices = np.arange(n_samples)\n",
    "            rng.shuffle(indices)\n",
    "            y = y_orig[indices]\n",
    "\n",
    "            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "            test_idx = test_folds[np.argsort(indices)] == 1\n",
    "            test = np.where(test_idx)[0]\n",
    "            train = np.where(~test_idx)[0]\n",
    "\n",
    "            yield train, test\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:41.223344Z",
     "iopub.status.busy": "2023-06-08T15:32:41.222954Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat \u001b[1m\u001b[34m#1\n",
      "Outer Loop fold 1, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23547\u001b[0m | Best iteration: \u001b[1m\u001b[34m 169\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.20031\u001b[0m | Best iteration: \u001b[1m\u001b[34m 134\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.32083\u001b[0m | Best iteration: \u001b[1m\u001b[34m 134\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22807\u001b[0m | Best iteration: \u001b[1m\u001b[34m 241\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.11357\u001b[0m | Best iteration: \u001b[1m\u001b[34m 404\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.22001\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.15895\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 2, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.36935\u001b[0m | Best iteration: \u001b[1m\u001b[34m 142\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.09748\u001b[0m | Best iteration: \u001b[1m\u001b[34m 331\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12947\u001b[0m | Best iteration: \u001b[1m\u001b[34m 257\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.29943\u001b[0m | Best iteration: \u001b[1m\u001b[34m 128\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16932\u001b[0m | Best iteration: \u001b[1m\u001b[34m 206\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21411\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21623\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 3, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16726\u001b[0m | Best iteration: \u001b[1m\u001b[34m 197\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21862\u001b[0m | Best iteration: \u001b[1m\u001b[34m 274\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13687\u001b[0m | Best iteration: \u001b[1m\u001b[34m 156\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.32646\u001b[0m | Best iteration: \u001b[1m\u001b[34m 134\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23190\u001b[0m | Best iteration: \u001b[1m\u001b[34m 206\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21591\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20138\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 4, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19490\u001b[0m | Best iteration: \u001b[1m\u001b[34m 124\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15006\u001b[0m | Best iteration: \u001b[1m\u001b[34m 238\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.28530\u001b[0m | Best iteration: \u001b[1m\u001b[34m 112\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14094\u001b[0m | Best iteration: \u001b[1m\u001b[34m 466\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21217\u001b[0m | Best iteration: \u001b[1m\u001b[34m 229\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19616\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.31355\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 5, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23563\u001b[0m | Best iteration: \u001b[1m\u001b[34m 121\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.25226\u001b[0m | Best iteration: \u001b[1m\u001b[34m 168\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.10986\u001b[0m | Best iteration: \u001b[1m\u001b[34m 521\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30338\u001b[0m | Best iteration: \u001b[1m\u001b[34m 101\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_46220/3944173749.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mperm_df_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importances_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfe_df_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_cv_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_cv_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m \u001b[0mperm_df_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_importances_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfe_df_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_cv_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouter_cv_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgbm_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermut\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_46220/3944173749.py\u001b[0m in \u001b[0;36mlgbm_tuning\u001b[0;34m(features, permut, rfe)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoostBoruta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'shap_importances'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_importance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                     model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n\u001b[0m\u001b[1;32m    153\u001b[0m                               eval_metric=bll_metric, early_stopping_rounds=300, verbose=-1)\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shaphypetune/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, trials, **fit_params)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shaphypetune/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, fit_params, params)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BoostSelector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredirect_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/shaphypetune/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0m_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_id_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mredirect_stdout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_fit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;31m# get coefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    965\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m         super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n\u001b[0m\u001b[1;32m    968\u001b[0m                     \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                     \u001b[0meval_class_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_init_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         self._Booster = train(\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks_after_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36meval_valid\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   3269\u001b[0m             \u001b[0mList\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3270\u001b[0m         \"\"\"\n\u001b[0;32m-> 3271\u001b[0;31m         return [item for i in range(1, self.__num_dataset)\n\u001b[0m\u001b[1;32m   3272\u001b[0m                 for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n\u001b[1;32m   3273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3270\u001b[0m         \"\"\"\n\u001b[1;32m   3271\u001b[0m         return [item for i in range(1, self.__num_dataset)\n\u001b[0;32m-> 3272\u001b[0;31m                 for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n\u001b[0m\u001b[1;32m   3273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'split'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   3807\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3808\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3809\u001b[0;31m                 \u001b[0mfeval_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3810\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_higher_better\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeval_ret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, preds, dataset)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \"\"\"\n\u001b[1;32m    173\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0margc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3245\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3246\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3247\u001b[0;31m     return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n\u001b[0m\u001b[1;32m   3248\u001b[0m                                    globals=globals, locals=locals, eval_str=eval_str)\n\u001b[1;32m   3249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2993\u001b[0m                       follow_wrapped=True, globals=None, locals=None, eval_str=False):\n\u001b[1;32m   2994\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0m\u001b[1;32m   2996\u001b[0m                                         \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2454\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2455\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2456\u001b[0;31m         return _signature_from_function(sigcls, obj,\n\u001b[0m\u001b[1;32m   2457\u001b[0m                                         \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2458\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2364\u001b[0m     \u001b[0;31m# Is 'func' is a pure Python function - don't validate the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m     \u001b[0;31m# parameters list (for correct order and defaults), it should be OK.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2366\u001b[0;31m     return cls(parameters,\n\u001b[0m\u001b[1;32m   2367\u001b[0m                \u001b[0mreturn_annotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2368\u001b[0m                __validate_parameters__=is_duck_function)\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parameters, return_annotation, __validate_parameters__)\u001b[0m\n\u001b[1;32m   2960\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2962\u001b[0;31m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMappingProxyType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2960\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2962\u001b[0;31m                 \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMappingProxyType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 4\n",
    "    n_folds = 5\n",
    "    num_boost_round = 10000\n",
    "    seeds = [1, 42, 228, 265, 21, 8081988, 5062023, 666, 1488]\n",
    "\n",
    "params = {\n",
    "        'boosting_type':'goss',\n",
    "        'learning_rate': 0.06733232950390658, \n",
    "        'n_estimators': 50000, \n",
    "        'early_stopping_round' : 100, \n",
    "        'subsample' : 0.6970532011679706,\n",
    "        'colsample_bytree': 0.6055755840633003,\n",
    "        'num_leaves': 6,\n",
    "        'class_weight': 'balanced',\n",
    "        'metric': 'none', \n",
    "        'is_unbalance': True, \n",
    "        'random_state': 8062023,\n",
    "        'feature_fraction_seed': 8062023,\n",
    "        'bagging_seed': 8062023,\n",
    "        'max_depth': 8,\n",
    "        'reg_alpha': 0.08866046540248787,  \n",
    "        'reg_lambda': 1.0245261859148395e-06,\n",
    "        'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability 𝑝 is replaced with max(min(𝑝,1−10−15),10−15)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1 - y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def calc_log_loss_weight(y_true): \n",
    "    '''w0, w1 assign different weights to individual data points during training.'''\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1\n",
    "\n",
    "def lgbm_tuning(features, permut=False, boruta=False):\n",
    "    metric = balanced_log_loss\n",
    "    eval_results_ = {}\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "    inner_cv_score = [] # store all cv scores of inner loop training\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        kf = MultilabelStratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=8062023+i)\n",
    "\n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X=train_df[features], y=greeks.iloc[:,1:3]), start = 1): \n",
    "            X, y = train_df[features], train_df.Class\n",
    "#             X, y = generated_features_train, train_df.Class\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            # 20% hold-out set\n",
    "            X_holdout, y_holdout = X_val, y_val\n",
    "\n",
    "            # Create an oof array for inner loop\n",
    "            oof_inner = np.zeros(len(X_train))\n",
    "\n",
    "            X_train = X_train.reset_index(drop=True)\n",
    "            y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "            cv = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=8062023+i) # Use stratifiedKfold to make life easier\n",
    "\n",
    "            X_outer, y_outer = X_train, y_train\n",
    "\n",
    "            models_ = [] # Used to store models trained in the inner loop.\n",
    "\n",
    "            print(f\"Outer Loop fold {fold}, Inner Loop Training with {blu}{X_train.shape[0]}{res} samples, {blu}{X_train.shape[1]}{res} features, seed = {blu}{8602023}{res}\")\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(cv.split(X=X_train, y=y_train), start = 1):\n",
    "                # Split the dataset according to the fold indexes.\n",
    "                X_train = X_outer.iloc[train_idx]\n",
    "                X_val = X_outer.iloc[val_idx]\n",
    "                y_train = y_outer.iloc[train_idx]\n",
    "                y_val = y_outer.iloc[val_idx]\n",
    "\n",
    "    #             trn_w0, trn_w1 = calc_log_loss_weight(y_train)\n",
    "    #             val_w0, val_w1 = calc_log_loss_weight(y_train)\n",
    "\n",
    "    #             w_val = [y_train.map({0: trn_w0, 1: trn_w1}), y_val.map({0: val_w0, 1: val_w1})]\n",
    "\n",
    "                eval_results_[fold]= {}\n",
    "\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "                clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                        eval_metric=bll_metric, # eval_sample_weight=w_val, \n",
    "                        early_stopping_rounds=300, verbose=-1)\n",
    "\n",
    "                models_.append(clf)\n",
    "\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                oof_inner[val_idx] = val_preds\n",
    "\n",
    "                val_score = metric(y_val, val_preds)\n",
    "                best_iter = clf.best_iteration_\n",
    "\n",
    "                print(f'Fold: {blu}{fold:>3}{res}| {metric.__name__}: {blu}{val_score:.5f}{res}'\n",
    "                      f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "                # permutation importance\n",
    "                if permut:\n",
    "                    perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                                 random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                    perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                       index=X_val.columns).sort_index()\n",
    "\n",
    "                    if perm_df_.shape[0] == 0:\n",
    "                        perm_df_ = perm_importance_df.copy()\n",
    "                    else:\n",
    "                        perm_df_ += perm_importance_df\n",
    "\n",
    "                # tree feature importance\n",
    "                f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                                  reverse=True, key=lambda x: x[1]), \n",
    "                                   columns=['Value','Feature'])\n",
    "\n",
    "                if feature_importances_.shape[0] == 0:\n",
    "                    feature_importances_ = f_i.copy()\n",
    "                else:\n",
    "\n",
    "                    feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "                # Boruta SHAP importance\n",
    "                if boruta:\n",
    "                    model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                              eval_metric=bll_metric, early_stopping_rounds=300, verbose=-1)\n",
    "                    \n",
    "                    boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                         index=X_train.columns).sort_index()\n",
    "                    if boruta_df_.shape[0] == 0:\n",
    "                        boruta_df_ = boruta_importance_df.copy()\n",
    "                    else:\n",
    "                        boruta_df_ += boruta_importance_df\n",
    "\n",
    "            mean_cv_score = metric(y_outer, oof_inner)\n",
    "            print(f'{red} Inner CV score: {res} {metric.__name__}: {red}{mean_cv_score:.5f}{res}')\n",
    "            print(f'{\"*\" * 50}\\n')\n",
    "            inner_cv_score.append(mean_cv_score)\n",
    "\n",
    "            # infer holdout data using 5-fold model trained in inner loop\n",
    "            preds = np.zeros(len(X_holdout))\n",
    "            for model in models_:\n",
    "                preds += model.predict_proba(X_holdout)[:,1]\n",
    "            preds = preds / len(models_)\n",
    "            cv_score = metric(y_holdout, preds)\n",
    "            print(f'{red} Outer Holdout score: {res} {metric.__name__}: {red}{cv_score:.5f}{res}')\n",
    "            print(f'{\"*\" * 50}\\n')\n",
    "            outer_cv_score.append(cv_score)\n",
    "\n",
    "    print(f'{red} Inner CV avg score: {res} {metric.__name__}: {red}{np.mean(inner_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} {metric.__name__}: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False)\n",
    "    \n",
    "        return perm_df_, feature_importances_, boruta_df_, np.mean(inner_cv_score), np.mean(outer_cv_score)\n",
    "\n",
    "perm_df_, feature_importances_, boruta_df_, inner_cv_score, outer_cv_score = lgbm_tuning(features, permut=False, boruta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate features by their score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:11.610606Z",
     "iopub.status.busy": "2023-06-08T15:32:11.610256Z",
     "iopub.status.idle": "2023-06-08T15:32:11.617365Z",
     "shell.execute_reply": "2023-06-08T15:32:11.614040Z",
     "shell.execute_reply.started": "2023-06-08T15:32:11.610577Z"
    }
   },
   "outputs": [],
   "source": [
    "# f_dict = dict()\n",
    "# features = list(features)\n",
    "\n",
    "# for i, f in tqdm(enumerate(features), total=len(features)):\n",
    "#     _, __, inner_cv_score, outer_cv_score = lgbm_tuning(features[:i] + features[i+1:], permut=False)\n",
    "#     f_dict[f] = (inner_cv_score, outer_cv_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Inner CV avg score:  balanced_log_loss: 0.21924\n",
    "**************************************************\n",
    "\n",
    " Outer Holdout avg score:  balanced_log_loss: 0.22748\n",
    "**************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze permutation feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:11.619184Z",
     "iopub.status.busy": "2023-06-08T15:32:11.618864Z",
     "iopub.status.idle": "2023-06-08T15:32:11.652915Z",
     "shell.execute_reply": "2023-06-08T15:32:11.652069Z",
     "shell.execute_reply.started": "2023-06-08T15:32:11.619157Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DU</th>\n",
       "      <td>4.933725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BQ</th>\n",
       "      <td>1.704082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AB</th>\n",
       "      <td>1.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CR</th>\n",
       "      <td>0.609565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CH</th>\n",
       "      <td>0.537724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EL</th>\n",
       "      <td>0.507318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CD</th>\n",
       "      <td>0.435890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CC</th>\n",
       "      <td>0.405793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DL</th>\n",
       "      <td>0.404865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AF</th>\n",
       "      <td>0.384457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GL</th>\n",
       "      <td>0.314471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DA</th>\n",
       "      <td>0.305504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EB</th>\n",
       "      <td>0.283859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EU</th>\n",
       "      <td>0.283756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CS</th>\n",
       "      <td>0.263245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DN</th>\n",
       "      <td>0.203463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR</th>\n",
       "      <td>0.202948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EE</th>\n",
       "      <td>0.202948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DY</th>\n",
       "      <td>0.202226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FI</th>\n",
       "      <td>0.193053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EP</th>\n",
       "      <td>0.192538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AM</th>\n",
       "      <td>0.172233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GE</th>\n",
       "      <td>0.162132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DI</th>\n",
       "      <td>0.142651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DE</th>\n",
       "      <td>0.112245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FD</th>\n",
       "      <td>0.091528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GF</th>\n",
       "      <td>0.061018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CU</th>\n",
       "      <td>0.060915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FS</th>\n",
       "      <td>0.050505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FL</th>\n",
       "      <td>0.040713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GH</th>\n",
       "      <td>0.040198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BN</th>\n",
       "      <td>0.010513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EG</th>\n",
       "      <td>0.010204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EJ</th>\n",
       "      <td>0.010204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH</th>\n",
       "      <td>0.000206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AZ</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DH</th>\n",
       "      <td>-0.029478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FC</th>\n",
       "      <td>-0.040301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BZ</th>\n",
       "      <td>-0.040507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BC</th>\n",
       "      <td>-0.050196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AX</th>\n",
       "      <td>-0.050608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BD</th>\n",
       "      <td>-0.070707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FE</th>\n",
       "      <td>-0.071429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    importance\n",
       "DU    4.933725\n",
       "BQ    1.704082\n",
       "AB    1.045455\n",
       "CR    0.609565\n",
       "CH    0.537724\n",
       "EL    0.507318\n",
       "CD    0.435890\n",
       "CC    0.405793\n",
       "DL    0.404865\n",
       "AF    0.384457\n",
       "GL    0.314471\n",
       "DA    0.305504\n",
       "EB    0.283859\n",
       "EU    0.283756\n",
       "CS    0.263245\n",
       "DN    0.203463\n",
       "FR    0.202948\n",
       "EE    0.202948\n",
       "DY    0.202226\n",
       "FI    0.193053\n",
       "EP    0.192538\n",
       "AM    0.172233\n",
       "GE    0.162132\n",
       "DI    0.142651\n",
       "DE    0.112245\n",
       "FD    0.091528\n",
       "GF    0.061018\n",
       "CU    0.060915\n",
       "FS    0.050505\n",
       "FL    0.040713\n",
       "GH    0.040198\n",
       "BN    0.010513\n",
       "EG    0.010204\n",
       "EJ    0.010204\n",
       "EH    0.000206\n",
       "AZ    0.000000\n",
       "DH   -0.029478\n",
       "FC   -0.040301\n",
       "BZ   -0.040507\n",
       "BC   -0.050196\n",
       "AX   -0.050608\n",
       "BD   -0.070707\n",
       "FE   -0.071429"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_df_.to_csv('perm_df.csv')\n",
    "perm_df_\n",
    "# perm_cols = set(perm_df_.index[-20:])\n",
    "# perm_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze tree gain feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:11.654626Z",
     "iopub.status.busy": "2023-06-08T15:32:11.654102Z",
     "iopub.status.idle": "2023-06-08T15:32:11.669674Z",
     "shell.execute_reply": "2023-06-08T15:32:11.668840Z",
     "shell.execute_reply.started": "2023-06-08T15:32:11.654583Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>129615.967042</td>\n",
       "      <td>AB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>105196.186197</td>\n",
       "      <td>DU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>75490.163484</td>\n",
       "      <td>BQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>51042.333952</td>\n",
       "      <td>CR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36022.696672</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>34456.751328</td>\n",
       "      <td>DA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31471.145397</td>\n",
       "      <td>GL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>30704.485973</td>\n",
       "      <td>AF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>29346.898976</td>\n",
       "      <td>CC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>27181.826056</td>\n",
       "      <td>FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>26143.141841</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24331.867667</td>\n",
       "      <td>EE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23202.932900</td>\n",
       "      <td>EU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23009.594052</td>\n",
       "      <td>EL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22593.607188</td>\n",
       "      <td>DL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22215.135397</td>\n",
       "      <td>FI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21443.563859</td>\n",
       "      <td>EB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>18331.784373</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18131.438185</td>\n",
       "      <td>DI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>16988.083227</td>\n",
       "      <td>BC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16026.672007</td>\n",
       "      <td>DN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15865.454910</td>\n",
       "      <td>DY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14750.477182</td>\n",
       "      <td>CH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12894.991660</td>\n",
       "      <td>EH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12003.232044</td>\n",
       "      <td>EP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11596.952697</td>\n",
       "      <td>FD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11239.855000</td>\n",
       "      <td>FE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10521.782608</td>\n",
       "      <td>DH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9663.864990</td>\n",
       "      <td>GH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9057.576167</td>\n",
       "      <td>FS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8927.609271</td>\n",
       "      <td>CU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8562.582726</td>\n",
       "      <td>BN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8027.775516</td>\n",
       "      <td>CS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6741.259262</td>\n",
       "      <td>AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6712.703324</td>\n",
       "      <td>FC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6108.024852</td>\n",
       "      <td>GF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5916.498473</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5674.099589</td>\n",
       "      <td>AX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5080.227791</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3961.750478</td>\n",
       "      <td>GE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3836.273721</td>\n",
       "      <td>BD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>325.860670</td>\n",
       "      <td>BZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>313.374259</td>\n",
       "      <td>EJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Value Feature\n",
       "42  129615.967042      AB\n",
       "20  105196.186197      DU\n",
       "34   75490.163484      BQ\n",
       "29   51042.333952      CR\n",
       "6    36022.696672      FL\n",
       "26   34456.751328      DA\n",
       "0    31471.145397      GL\n",
       "41   30704.485973      AF\n",
       "32   29346.898976      CC\n",
       "5    27181.826056      FR\n",
       "31   26143.141841      CD\n",
       "17   24331.867667      EE\n",
       "11   23202.932900      EU\n",
       "13   23009.594052      EL\n",
       "22   22593.607188      DL\n",
       "7    22215.135397      FI\n",
       "18   21443.563859      EB\n",
       "25   18331.784373      DE\n",
       "23   18131.438185      DI\n",
       "37   16988.083227      BC\n",
       "21   16026.672007      DN\n",
       "19   15865.454910      DY\n",
       "30   14750.477182      CH\n",
       "15   12894.991660      EH\n",
       "12   12003.232044      EP\n",
       "9    11596.952697      FD\n",
       "8    11239.855000      FE\n",
       "24   10521.782608      DH\n",
       "1     9663.864990      GH\n",
       "4     9057.576167      FS\n",
       "27    8927.609271      CU\n",
       "35    8562.582726      BN\n",
       "28    8027.775516      CS\n",
       "40    6741.259262      AM\n",
       "10    6712.703324      FC\n",
       "2     6108.024852      GF\n",
       "16    5916.498473      EG\n",
       "39    5674.099589      AX\n",
       "38    5080.227791      AZ\n",
       "3     3961.750478      GE\n",
       "36    3836.273721      BD\n",
       "33     325.860670      BZ\n",
       "14     313.374259      EJ"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances_.to_csv('feature_importances.csv')\n",
    "feature_importances_\n",
    "# fi_cols = set(feature_importances_['Feature'].values[-20:])\n",
    "# fi_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze BORUTA importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_df_.to_csv('perm_df.csv')\n",
    "boruta_df_\n",
    "boruta_cols = set(boruta_df_.index[-20:])\n",
    "boruta_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:11.672883Z",
     "iopub.status.busy": "2023-06-08T15:32:11.672350Z",
     "iopub.status.idle": "2023-06-08T15:32:38.349383Z",
     "shell.execute_reply": "2023-06-08T15:32:38.347823Z",
     "shell.execute_reply.started": "2023-06-08T15:32:11.672854Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m1\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.08236\u001b[0m | Best iteration: \u001b[1m\u001b[34m 714\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.33222\u001b[0m | Best iteration: \u001b[1m\u001b[34m 106\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14185\u001b[0m | Best iteration: \u001b[1m\u001b[34m 157\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17615\u001b[0m | Best iteration: \u001b[1m\u001b[34m 260\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13430\u001b[0m | Best iteration: \u001b[1m\u001b[34m 401\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.17731\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m42\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30060\u001b[0m | Best iteration: \u001b[1m\u001b[34m 109\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17532\u001b[0m | Best iteration: \u001b[1m\u001b[34m 329\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.08776\u001b[0m | Best iteration: \u001b[1m\u001b[34m 460\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12859\u001b[0m | Best iteration: \u001b[1m\u001b[34m 477\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.20258\u001b[0m | Best iteration: \u001b[1m\u001b[34m 156\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.17870\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m228\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.18694\u001b[0m | Best iteration: \u001b[1m\u001b[34m 223\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22283\u001b[0m | Best iteration: \u001b[1m\u001b[34m 266\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16458\u001b[0m | Best iteration: \u001b[1m\u001b[34m 367\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.10262\u001b[0m | Best iteration: \u001b[1m\u001b[34m 941\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24894\u001b[0m | Best iteration: \u001b[1m\u001b[34m 149\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18102\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m265\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.36829\u001b[0m | Best iteration: \u001b[1m\u001b[34m 135\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17863\u001b[0m | Best iteration: \u001b[1m\u001b[34m 184\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16826\u001b[0m | Best iteration: \u001b[1m\u001b[34m 240\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.10073\u001b[0m | Best iteration: \u001b[1m\u001b[34m 369\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16038\u001b[0m | Best iteration: \u001b[1m\u001b[34m 245\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19488\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m21\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15389\u001b[0m | Best iteration: \u001b[1m\u001b[34m 171\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17875\u001b[0m | Best iteration: \u001b[1m\u001b[34m 236\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19694\u001b[0m | Best iteration: \u001b[1m\u001b[34m 273\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.39039\u001b[0m | Best iteration: \u001b[1m\u001b[34m  93\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17027\u001b[0m | Best iteration: \u001b[1m\u001b[34m 274\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19963\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m8081988\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21788\u001b[0m | Best iteration: \u001b[1m\u001b[34m 131\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.20253\u001b[0m | Best iteration: \u001b[1m\u001b[34m 164\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.09087\u001b[0m | Best iteration: \u001b[1m\u001b[34m 624\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.18757\u001b[0m | Best iteration: \u001b[1m\u001b[34m 157\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.29486\u001b[0m | Best iteration: \u001b[1m\u001b[34m 141\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20441\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m5062023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.18599\u001b[0m | Best iteration: \u001b[1m\u001b[34m 169\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30076\u001b[0m | Best iteration: \u001b[1m\u001b[34m 246\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12070\u001b[0m | Best iteration: \u001b[1m\u001b[34m 293\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.25052\u001b[0m | Best iteration: \u001b[1m\u001b[34m 137\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12139\u001b[0m | Best iteration: \u001b[1m\u001b[34m 386\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19768\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m666\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.18102\u001b[0m | Best iteration: \u001b[1m\u001b[34m 223\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30448\u001b[0m | Best iteration: \u001b[1m\u001b[34m 105\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.29240\u001b[0m | Best iteration: \u001b[1m\u001b[34m 120\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.09905\u001b[0m | Best iteration: \u001b[1m\u001b[34m 474\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.11081\u001b[0m | Best iteration: \u001b[1m\u001b[34m 343\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19904\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m43\u001b[0m features, seed = \u001b[1m\u001b[34m1488\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14668\u001b[0m | Best iteration: \u001b[1m\u001b[34m 232\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17500\u001b[0m | Best iteration: \u001b[1m\u001b[34m 245\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24203\u001b[0m | Best iteration: \u001b[1m\u001b[34m 158\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.27003\u001b[0m | Best iteration: \u001b[1m\u001b[34m 143\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16624\u001b[0m | Best iteration: \u001b[1m\u001b[34m 184\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20181\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss for all seeds: \u001b[1m\u001b[31m0.19272\n"
     ]
    }
   ],
   "source": [
    "def lgbm_training():\n",
    "    models_ = list()\n",
    "    all_cv_score = list()\n",
    "    weights_ = list()\n",
    "    \n",
    "    all_eval_results_ = dict() # used to store evaluation results for each seed\n",
    "    X, y = train_df[features], train_df.Class\n",
    "#     X, y = generated_features_train, train_df.Class\n",
    "    \n",
    "    for seed in CFG.seeds:   \n",
    "        kf = MultilabelStratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=seed)\n",
    "        metric = balanced_log_loss\n",
    "        eval_results_ = {}     # used to store evaluation results for each fold\n",
    "        oof = np.zeros(len(X))\n",
    "        print(f\"Training with {blu}{X.shape[1]}{res} features, seed = {blu}{seed}{res}\")\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(kf.split(X=train_df, y=greeks.iloc[:,1:3]), start = 1):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "#             trn_w0, trn_w1 = calc_log_loss_weight(y_train)\n",
    "#             val_w0, val_w1 = calc_log_loss_weight(y_val)\n",
    "\n",
    "#             w_val = [y_train.map({0: trn_w0, 1: trn_w1}), y_val.map({0: val_w0, 1: val_w1})]\n",
    "\n",
    "            clf = lgb.LGBMClassifier(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=bll_metric, early_stopping_rounds=300,\n",
    "                    verbose=-1)\n",
    "\n",
    "            eval_results_[fold]= clf.evals_result_\n",
    "            \n",
    "            val_preds = clf.predict_proba(X_val)[:,1]\n",
    "            oof[val_idx] = val_preds\n",
    "\n",
    "            val_score = balanced_log_loss(y_val, val_preds)\n",
    "            best_iter = clf.best_iteration_\n",
    "            \n",
    "            print(f'Fold: {blu}{fold:>3}{res}| {metric.__name__}: {blu}{val_score:.5f}{res}'\n",
    "                  f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # Stores the model\n",
    "            models_.append(clf)\n",
    "            \n",
    "            # Stores evaluation result\n",
    "            all_eval_results_[seed] = eval_results_\n",
    "            \n",
    "            # Store prediction weight\n",
    "            weights_.append(1/clf.evals_result_['valid_1']['balanced_log_loss'][clf.best_iteration_])\n",
    "            \n",
    "        mean_cv_score = metric(y, oof)\n",
    "        all_cv_score.append(mean_cv_score)\n",
    "        print(f'{red}Mean{res} {metric.__name__}: {red}{mean_cv_score:.5f}')\n",
    "        print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    print(f'{red}Mean{res} {metric.__name__} for all seeds: {red}{np.mean(all_cv_score):.5f}')\n",
    "    return models_, eval_results_, all_eval_results_, weights_\n",
    "\n",
    "models_, eval_results_, all_eval_results_, weights_ = lgbm_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T15:32:38.350522Z",
     "iopub.status.idle": "2023-06-08T15:32:38.351460Z",
     "shell.execute_reply": "2023-06-08T15:32:38.351239Z",
     "shell.execute_reply.started": "2023-06-08T15:32:38.351217Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Model Evaluation\n",
    "# metric_score_folds = pd.DataFrame.from_dict(all_eval_results_)\n",
    "# fit_logloss = []\n",
    "# val_logloss = []\n",
    "\n",
    "# for seed in CFG.seeds:\n",
    "#     for fold in range(1,CFG.n_folds+1):\n",
    "#         fit_logloss.append(metric_score_folds[seed][fold]['training']['balanced_log_loss'])\n",
    "#         val_logloss.append(metric_score_folds[seed][fold]['valid_1']['balanced_log_loss'])\n",
    "\n",
    "# fig, axes = plt.subplots(math.ceil(CFG.n_folds*len(CFG.seeds)/CFG.n_folds), CFG.n_folds, figsize=(20, 20), dpi=150)\n",
    "# ax = axes.flatten()\n",
    "# for i, (f, v, m) in enumerate(zip(fit_logloss, val_logloss, models_), start = 1): \n",
    "#     sns.lineplot(f, color='#B90000', ax=ax[i-1], label='fit')\n",
    "#     sns.lineplot(v, color='#048BA8', ax=ax[i-1], label='val')\n",
    "#     ax[i-1].legend()\n",
    "#     ax[i-1].spines['top'].set_visible(False);\n",
    "#     ax[i-1].spines['right'].set_visible(False)\n",
    "#     ax[i-1].set_title(f'Seed {CFG.seeds[(i-1)//CFG.n_folds]} Fold {CFG.n_folds if i%CFG.n_folds==0 else i%CFG.n_folds}', fontdict={'fontweight': 'bold'})\n",
    "\n",
    "#     color =  ['#048BA8', palette[-3]]\n",
    "#     best_iter = m.best_iteration_\n",
    "#     span_range = [[0, best_iter], [best_iter + 10, best_iter + CFG.num_boost_round]]\n",
    "\n",
    "#     for idx, sub_title in enumerate([f'Best\\nIteration: {best_iter}', f'Early\\n Stopping: 2000']):\n",
    "#         ax[i-1].annotate(sub_title,\n",
    "#                     xy=(sum(span_range[idx])/2 , 0.5),\n",
    "#                     xytext=(0,0), textcoords='offset points',\n",
    "#                     va=\"center\", ha=\"center\",\n",
    "#                     color=\"w\", fontsize=16, fontweight='bold',\n",
    "#                     bbox=dict(boxstyle='round4', pad=0.4, color=color[idx], alpha=0.6))\n",
    "#         ax[i-1].axvspan(span_range[idx][0]-0.4,span_range[idx][1]+0.4,  color=color[idx], alpha=0.07)\n",
    "\n",
    "#     ax[i-1].set_xlim(0, best_iter + 20 + 2000)\n",
    "#     ax[i-1].legend(bbox_to_anchor=(0.95, 1), loc='upper right', title='logloss')\n",
    "\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T15:32:38.352845Z",
     "iopub.status.idle": "2023-06-08T15:32:38.353222Z",
     "shell.execute_reply": "2023-06-08T15:32:38.353059Z",
     "shell.execute_reply.started": "2023-06-08T15:32:38.353042Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generated_features_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27995/1129942244.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# predictions = predict(test_df[features])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_features_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class_1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_features_test' is not defined"
     ]
    }
   ],
   "source": [
    "# test_preds = np.zeros((test_df.shape[0],2))\n",
    "\n",
    "# for i in range(len(final_test_predictions)):\n",
    "#     test_preds[:, 0] += test_weights[i] * final_test_predictions[i][:, 0]\n",
    "#     test_preds[:, 1] += test_weights[i] * final_test_predictions[i][:, 1]\n",
    "\n",
    "# test_preds /= sum(test_weights)\n",
    "\n",
    "def predict(X):\n",
    "    y = np.zeros(len(X))\n",
    "    for i, model in enumerate(models_):\n",
    "#         y += weights_[i] * model.predict_proba(X)[:,1]\n",
    "        y += model.predict_proba(X)[:,1]\n",
    "#     return y / sum(weights_)\n",
    "    return y / len(models_)\n",
    "\n",
    "# predictions = predict(test_df[features])\n",
    "predictions = predict(generated_features_test)\n",
    "\n",
    "test_df['class_1'] = predictions\n",
    "test_df['class_0'] = 1 - predictions\n",
    "\n",
    "sample_submission[['class_0', 'class_1']] = test_df[['class_0', 'class_1']]\n",
    "sample_submission.to_csv(r\"submission.csv\", index=False)\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a lot of resulting features. I have already identified a few important once. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
