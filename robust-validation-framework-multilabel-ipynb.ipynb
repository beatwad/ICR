{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:23.665663Z",
     "iopub.status.busy": "2023-06-08T15:31:23.665223Z",
     "iopub.status.idle": "2023-06-08T15:31:35.997437Z",
     "shell.execute_reply": "2023-06-08T15:31:35.996163Z",
     "shell.execute_reply.started": "2023-06-08T15:31:23.665630Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, BaseShuffleSplit, _validate_shuffle_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.validation import _num_samples, check_array\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "import eli5\n",
    "from IPython.display import display\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:36.001085Z",
     "iopub.status.busy": "2023-06-08T15:31:35.999808Z",
     "iopub.status.idle": "2023-06-08T15:31:36.066222Z",
     "shell.execute_reply": "2023-06-08T15:31:36.064862Z",
     "shell.execute_reply.started": "2023-06-08T15:31:36.001050Z"
    }
   },
   "outputs": [],
   "source": [
    "# COMP_PATH = \"/kaggle/input/icr-identify-age-related-conditions\"\n",
    "COMP_PATH = \"icr-identify-age-related-conditions\"\n",
    "\n",
    "train_df = pd.read_csv(f'{COMP_PATH}//train.csv')\n",
    "test_df = pd.read_csv(f'{COMP_PATH}/test.csv')\n",
    "greeks = pd.read_csv(f\"{COMP_PATH}/greeks.csv\")\n",
    "sample_submission = pd.read_csv(f\"{COMP_PATH}/sample_submission.csv\")\n",
    "\n",
    "train_df['EJ'] = train_df['EJ'].replace({'A': 0, 'B': 1})\n",
    "test_df['EJ'] = test_df['EJ'].replace({'A': 0, 'B': 1})\n",
    "\n",
    "train_df.columns = train_df.columns.str.replace(' ', '')\n",
    "test_df.columns = test_df.columns.str.replace(' ', '')\n",
    "\n",
    "# train_df.drop('Id',axis=1, inplace=True)\n",
    "# train_df.fillna(train_df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "features_with_outliers = [fe for fe in train_df.columns if fe not in ['BN', 'BQ', 'CW', 'EL', 'GH', \n",
    "                                                                      'GI', 'GL', 'Id', 'Class', 'EJ']]\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "for f in features_with_outliers:\n",
    "    train_df[f] = train_df[f].clip(upper=train_df[f].quantile(0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:36.069293Z",
     "iopub.status.busy": "2023-06-08T15:31:36.068506Z",
     "iopub.status.idle": "2023-06-08T15:31:36.075275Z",
     "shell.execute_reply": "2023-06-08T15:31:36.074052Z",
     "shell.execute_reply.started": "2023-06-08T15:31:36.069253Z"
    }
   },
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# new_num_cols = train_df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# train_df[new_num_cols] = scaler.fit_transform(train_df[new_num_cols])\n",
    "# test_df[new_num_cols] = scaler.transform(test_df[new_num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine features in all possible ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi = pd.read_csv('feature_importances.csv', index_col = 'Unnamed: 0')\n",
    "# fi_cols = set(fi['Feature'].head(100).values)\n",
    "\n",
    "# perm = pd.read_csv('perm_df.csv', index_col = 'Unnamed: 0')\n",
    "# perm_cols = set(perm['importance'].head(100).index)\n",
    "\n",
    "# important_col = list(perm_cols.intersection(fi_cols))\n",
    "# print(important_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [fe for fe in train_df.columns if fe not in ['Id','CF', 'CB', 'DV', 'BR', 'DF', 'AR', 'GI', 'AY', 'GB',\n",
    "#                                                         'AH', 'CW', 'CL', 'Class', 'BP']]\n",
    "\n",
    "# for f in features:\n",
    "#     train_df[f] = np.floor(train_df[f]*1000)/1000 # quality decreases no significant result for LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log features (preserve sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in features:\n",
    "#     train_df[f] = np.sign(train_df[f]) * np.log1p(np.abs(train_df[f])) # no significant result for LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:36.080972Z",
     "iopub.status.busy": "2023-06-08T15:31:36.079281Z",
     "iopub.status.idle": "2023-06-08T15:31:44.412385Z",
     "shell.execute_reply": "2023-06-08T15:31:44.411508Z",
     "shell.execute_reply.started": "2023-06-08T15:31:36.080912Z"
    }
   },
   "outputs": [],
   "source": [
    "# features = train_df.drop(['Class', 'Id'], axis=1).columns\n",
    "\n",
    "features = [fe for fe in train_df.columns if fe not in ['Id','Class']]\n",
    "\n",
    "features = [fe for fe in train_df.columns if fe not in ['Id','CF', 'CB', 'DV', 'BR', 'DF', 'AR', 'GI', 'AY', 'GB',\n",
    "                                                        'AH', 'CW', 'CL', 'Class', 'BP', 'AX', 'AZ', 'BD', 'BZ', 'EG', \n",
    "                                                        'EH', 'EJ', 'FC', 'FD', 'FE', 'FS', 'GF', 'GH']]\n",
    "\n",
    "# ['AX', 'AZ', 'BD', 'BZ', 'EG', 'EH', 'EJ', 'FC', 'FD', 'FE', 'FS', 'GF', 'GH']\n",
    "\n",
    "# selected = ['CR+DU', 'EH-FI', 'CD-DL', 'AB-FI', 'FL+GL', 'DU-EP', 'BQ+EP', 'FR+GL', 'CD-EP', 'AF-EG', \n",
    "#             'CU-DU', 'AB-CR', 'BQ+CD', 'BQ-GF', 'BQ+DI', 'CR+DH', 'CD+EL', 'CC+EH', 'DA-DU', 'AF+BQ', \n",
    "#             'CR+EH', 'BQ-DN', 'DN-EL', 'EJ+FI', 'BQ-EP', 'EE*EP', 'BQ/CU', 'AB*FR', 'EJ/FL', 'CD/CH', \n",
    "#             'CD/EP', 'CR*DU', 'AF/EG', 'CC/CD', 'AB/CH', 'BQ/CH', 'CH/DI', 'AB/DN', 'DU/EP', 'DU/EJ', \n",
    "#             'DU/GL', 'DY/EE', 'CR*CS', 'DH/DU', 'EJ*GL', 'CD/DL', 'DH/DI', 'DU*FR', 'CR*DH']\n",
    "\n",
    "# def gen_features(features, df):\n",
    "#     generated_features = pd.DataFrame()\n",
    "\n",
    "#     for fe_a, fe_b in tqdm(itertools.combinations(features, 2), total=sum([1 for i in itertools.combinations(features, 2)])):\n",
    "#         generated_features[f'{fe_a}+{fe_b}']   = df[fe_a] + df[fe_b]\n",
    "#         generated_features[f'{fe_a}-{fe_b}']   = df[fe_a] - df[fe_b] \n",
    "#         generated_features[f'{fe_a}*{fe_b}']   = df[fe_a] * df[fe_b]\n",
    "#         generated_features[f'{fe_a}/{fe_b}']   = df[fe_a] / df[fe_b]\n",
    "\n",
    "# #         generated_features[f'{fe_a}_2']        = df[fe_a].pow(2)\n",
    "# #         generated_features[f'{fe_b}_2']        = df[fe_b].pow(2)\n",
    "# #         generated_features[f'{fe_a}*{fe_b}_2'] = df[fe_a] * df[fe_b].pow(2)\n",
    "# #         generated_features[f'{fe_a}_2*{fe_b}'] = df[fe_a].pow(2) * df[fe_b]\n",
    "\n",
    "# #         generated_features[f'{fe_a}_05'] = df[fe_a].pow(0.5)\n",
    "# #         generated_features[f'{fe_b}_05'] = df[fe_b].pow(0.5)\n",
    "# #         generated_features[f'{fe_a}*{fe_b}_05'] = df[fe_a] * df[fe_b].pow(0.5)\n",
    "# #         generated_features[f'{fe_a}_05*{fe_b}'] = df[fe_a].pow(0.5) * df[fe_b]\n",
    "\n",
    "# #         generated_features[f'{fe_a}_log'] = np.log(df[fe_a])\n",
    "# #         generated_features[f'{fe_b}_log'] = np.log(df[fe_b])\n",
    "# #         generated_features[f'{fe_a}*{fe_b}_log'] = df[fe_a] * np.log(df[fe_b])\n",
    "# #         generated_features[f'{fe_a}_log*{fe_b}'] = np.log(df[fe_a]) * df[fe_b]\n",
    "        \n",
    "#     generated_features = generated_features[selected]\n",
    "#     generated_features = pd.concat([generated_features, df[features]], axis=1)\n",
    "    \n",
    "#     # prevent inf\n",
    "#     for g in generated_features.columns:\n",
    "#         generated_features[g] = np.minimum(np.maximum(generated_features[g], -1e9), 1e9)\n",
    "    \n",
    "#     return generated_features\n",
    "\n",
    "# generated_features_train = gen_features(features, train_df)\n",
    "# generated_features_test = gen_features(features, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:44.414098Z",
     "iopub.status.busy": "2023-06-08T15:31:44.413784Z",
     "iopub.status.idle": "2023-06-08T15:31:44.461531Z",
     "shell.execute_reply": "2023-06-08T15:31:44.460184Z",
     "shell.execute_reply.started": "2023-06-08T15:31:44.414071Z"
    }
   },
   "outputs": [],
   "source": [
    "def IterativeStratification(labels, r, random_state):\n",
    "    \"\"\"This function implements the Iterative Stratification algorithm described\n",
    "    in the following paper:\n",
    "    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n",
    "    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n",
    "    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n",
    "    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n",
    "    Heidelberg.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = labels.shape[0]\n",
    "    test_folds = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    # Calculate the desired number of examples at each subset\n",
    "    c_folds = r * n_samples\n",
    "\n",
    "    # Calculate the desired number of examples of each label at each subset\n",
    "    c_folds_labels = np.outer(r, labels.sum(axis=0))\n",
    "\n",
    "    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n",
    "\n",
    "    while np.any(labels_not_processed_mask):\n",
    "        # Find the label with the fewest (but at least one) remaining examples,\n",
    "        # breaking ties randomly\n",
    "        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n",
    "\n",
    "        # Handle case where only all-zero labels are left by distributing\n",
    "        # across all folds as evenly as possible (not in original algorithm but\n",
    "        # mentioned in the text). (By handling this case separately, some\n",
    "        # code redundancy is introduced; however, this approach allows for\n",
    "        # decreased execution time when there are a relatively large number\n",
    "        # of all-zero labels.)\n",
    "        if num_labels.sum() == 0:\n",
    "            sample_idxs = np.where(labels_not_processed_mask)[0]\n",
    "\n",
    "            for sample_idx in sample_idxs:\n",
    "                fold_idx = np.where(c_folds == c_folds.max())[0]\n",
    "\n",
    "                if fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n",
    "\n",
    "                test_folds[sample_idx] = fold_idx\n",
    "                c_folds[fold_idx] -= 1\n",
    "\n",
    "            break\n",
    "\n",
    "        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n",
    "        if label_idx.shape[0] > 1:\n",
    "            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n",
    "\n",
    "        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n",
    "\n",
    "        for sample_idx in sample_idxs:\n",
    "            # Find the subset(s) with the largest number of desired examples\n",
    "            # for this label, breaking ties by considering the largest number\n",
    "            # of desired examples, breaking further ties randomly\n",
    "            label_folds = c_folds_labels[:, label_idx]\n",
    "            fold_idx = np.where(label_folds == label_folds.max())[0]\n",
    "\n",
    "            if fold_idx.shape[0] > 1:\n",
    "                temp_fold_idx = np.where(c_folds[fold_idx] ==\n",
    "                                         c_folds[fold_idx].max())[0]\n",
    "                fold_idx = fold_idx[temp_fold_idx]\n",
    "\n",
    "                if temp_fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n",
    "\n",
    "            test_folds[sample_idx] = fold_idx\n",
    "            labels_not_processed_mask[sample_idx] = False\n",
    "\n",
    "            # Update desired number of examples\n",
    "            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n",
    "            c_folds[fold_idx] -= 1\n",
    "\n",
    "    return test_folds\n",
    "\n",
    "\n",
    "class MultilabelStratifiedKFold(_BaseKFold):\n",
    "    \"\"\"Multilabel stratified K-Folds cross-validator\n",
    "    Provides train/test indices to split multilabel data into train/test sets.\n",
    "    This cross-validation object is a variation of KFold that returns\n",
    "    stratified folds for multilabel data. The folds are made by preserving\n",
    "    the percentage of samples for each label.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=3\n",
    "        Number of folds. Must be at least 2.\n",
    "    shuffle : boolean, optional\n",
    "        Whether to shuffle each stratification of the data before splitting\n",
    "        into batches.\n",
    "    random_state : int, RandomState instance or None, optional, default=None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedKFold that only uses random_state\n",
    "        when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n",
    "    >>> mskf.get_n_splits(X, y)\n",
    "    2\n",
    "    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n",
    "    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n",
    "    >>> for train_index, test_index in mskf.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different in each fold.\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=3, *, shuffle=False, random_state=None):\n",
    "        super(MultilabelStratifiedKFold, self).__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def _make_test_folds(self, X, y):\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n",
    "\n",
    "        num_samples = y.shape[0]\n",
    "\n",
    "        rng = check_random_state(self.random_state)\n",
    "        indices = np.arange(num_samples)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(indices)\n",
    "            y = y[indices]\n",
    "\n",
    "        r = np.asarray([1 / self.n_splits] * self.n_splits)\n",
    "\n",
    "        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "        return test_folds[np.argsort(indices)]\n",
    "\n",
    "    def _iter_test_masks(self, X=None, y=None, groups=None):\n",
    "        test_folds = self._make_test_folds(X, y)\n",
    "        for i in range(self.n_splits):\n",
    "            yield test_folds == i\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n",
    "\n",
    "\n",
    "class RepeatedMultilabelStratifiedKFold(_RepeatedSplits):\n",
    "    \"\"\"Repeated Multilabel Stratified K-Fold cross validator.\n",
    "    Repeats Mulilabel Stratified K-Fold n times with different randomization\n",
    "    in each repetition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of folds. Must be at least 2.\n",
    "    n_repeats : int, default=10\n",
    "        Number of times cross-validator needs to be repeated.\n",
    "    random_state : None, int or RandomState, default=None\n",
    "        Random state to be used to generate random state for each\n",
    "        repetition as well as randomly breaking ties within the iterative\n",
    "        stratification algorithm.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> rmskf = RepeatedMultilabelStratifiedKFold(n_splits=2, n_repeats=2,\n",
    "    ...     random_state=0)\n",
    "    >>> for train_index, test_index in rmskf.split(X, y):\n",
    "    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...     X_train, X_test = X[train_index], X[test_index]\n",
    "    ...     y_train, y_test = y[train_index], y[test_index]\n",
    "    ...\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [0 1 4 5] TEST: [2 3 6 7]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedStratifiedKFold: Repeats (Non-multilabel) Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, *, n_repeats=10, random_state=None):\n",
    "        super(RepeatedMultilabelStratifiedKFold, self).__init__(\n",
    "            MultilabelStratifiedKFold, n_repeats=n_repeats, random_state=random_state,\n",
    "            n_splits=n_splits)\n",
    "\n",
    "\n",
    "class MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n",
    "    \"\"\"Multilabel Stratified ShuffleSplit cross-validator\n",
    "    Provides train/test indices to split data into train/test sets.\n",
    "    This cross-validation object is a merge of MultilabelStratifiedKFold and\n",
    "    ShuffleSplit, which returns stratified randomized folds for multilabel\n",
    "    data. The folds are made by preserving the percentage of each label.\n",
    "    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n",
    "    do not guarantee that all folds will be different, although this is\n",
    "    still very likely for sizeable datasets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default 10\n",
    "        Number of re-shuffling & splitting iterations.\n",
    "    test_size : float, int, None, optional\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split. If int, represents the\n",
    "        absolute number of test samples. If None, the value is set to the\n",
    "        complement of the train size. By default, the value is set to 0.1.\n",
    "        The default will change in version 0.21. It will remain 0.1 only\n",
    "        if ``train_size`` is unspecified, otherwise it will complement\n",
    "        the specified ``train_size``.\n",
    "    train_size : float, int, or None, default is None\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedShuffleSplit that only uses\n",
    "        random_state when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n",
    "    ...    random_state=0)\n",
    "    >>> msss.get_n_splits(X, y)\n",
    "    3\n",
    "    >>> print(mss)       # doctest: +ELLIPSIS\n",
    "    MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n",
    "                                     train_size=None)\n",
    "    >>> for train_index, test_index in msss.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different from desired due to the\n",
    "    preference of stratification over perfectly sized folds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=10, *, test_size=\"default\", train_size=None,\n",
    "                 random_state=None):\n",
    "        super(MultilabelStratifiedShuffleSplit, self).__init__(\n",
    "            n_splits=n_splits, test_size=test_size, train_size=train_size, random_state=random_state)\n",
    "\n",
    "    def _iter_indices(self, X, y, groups=None):\n",
    "        n_samples = _num_samples(X)\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n",
    "                    type_of_target_y))\n",
    "\n",
    "        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n",
    "                                                  self.train_size)\n",
    "\n",
    "        n_samples = y.shape[0]\n",
    "        rng = check_random_state(self.random_state)\n",
    "        y_orig = y.copy()\n",
    "\n",
    "        r = np.array([n_train, n_test]) / (n_train + n_test)\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            indices = np.arange(n_samples)\n",
    "            rng.shuffle(indices)\n",
    "            y = y_orig[indices]\n",
    "\n",
    "            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "            test_idx = test_folds[np.argsort(indices)] == 1\n",
    "            test = np.where(test_idx)[0]\n",
    "            train = np.where(~test_idx)[0]\n",
    "\n",
    "            yield train, test\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:41.223344Z",
     "iopub.status.busy": "2023-06-08T15:32:41.222954Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat \u001b[1m\u001b[34m#1\n",
      "Outer Loop fold 1, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22666\u001b[0m | Best iteration: \u001b[1m\u001b[34m 176\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.18390\u001b[0m | Best iteration: \u001b[1m\u001b[34m 122\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.29212\u001b[0m | Best iteration: \u001b[1m\u001b[34m 176\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.20285\u001b[0m | Best iteration: \u001b[1m\u001b[34m 239\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12348\u001b[0m | Best iteration: \u001b[1m\u001b[34m 277\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20597\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.16362\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 2, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.35666\u001b[0m | Best iteration: \u001b[1m\u001b[34m 206\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.09546\u001b[0m | Best iteration: \u001b[1m\u001b[34m 408\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19520\u001b[0m | Best iteration: \u001b[1m\u001b[34m 136\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.28382\u001b[0m | Best iteration: \u001b[1m\u001b[34m 161\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15390\u001b[0m | Best iteration: \u001b[1m\u001b[34m 334\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21841\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18477\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 3, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15585\u001b[0m | Best iteration: \u001b[1m\u001b[34m 202\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19720\u001b[0m | Best iteration: \u001b[1m\u001b[34m 279\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.10203\u001b[0m | Best iteration: \u001b[1m\u001b[34m 176\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30246\u001b[0m | Best iteration: \u001b[1m\u001b[34m 158\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22829\u001b[0m | Best iteration: \u001b[1m\u001b[34m 131\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19686\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20224\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 4, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19031\u001b[0m | Best iteration: \u001b[1m\u001b[34m  91\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14732\u001b[0m | Best iteration: \u001b[1m\u001b[34m 442\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.28432\u001b[0m | Best iteration: \u001b[1m\u001b[34m 118\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.11105\u001b[0m | Best iteration: \u001b[1m\u001b[34m 492\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17442\u001b[0m | Best iteration: \u001b[1m\u001b[34m 189\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18144\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.31857\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 5, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23431\u001b[0m | Best iteration: \u001b[1m\u001b[34m 141\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.25098\u001b[0m | Best iteration: \u001b[1m\u001b[34m 192\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.09847\u001b[0m | Best iteration: \u001b[1m\u001b[34m 364\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.33510\u001b[0m | Best iteration: \u001b[1m\u001b[34m 104\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24810\u001b[0m | Best iteration: \u001b[1m\u001b[34m 138\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.23353\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21257\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Repeat \u001b[1m\u001b[34m#2\n",
      "Outer Loop fold 1, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.20894\u001b[0m | Best iteration: \u001b[1m\u001b[34m 191\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30193\u001b[0m | Best iteration: \u001b[1m\u001b[34m 101\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12457\u001b[0m | Best iteration: \u001b[1m\u001b[34m 428\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21932\u001b[0m | Best iteration: \u001b[1m\u001b[34m 189\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.29234\u001b[0m | Best iteration: \u001b[1m\u001b[34m 121\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.22885\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.17401\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 2, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.20135\u001b[0m | Best iteration: \u001b[1m\u001b[34m 192\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13674\u001b[0m | Best iteration: \u001b[1m\u001b[34m 259\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.05884\u001b[0m | Best iteration: \u001b[1m\u001b[34m 566\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21152\u001b[0m | Best iteration: \u001b[1m\u001b[34m 239\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19854\u001b[0m | Best iteration: \u001b[1m\u001b[34m 204\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.16086\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.38326\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 3, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24830\u001b[0m | Best iteration: \u001b[1m\u001b[34m 180\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19614\u001b[0m | Best iteration: \u001b[1m\u001b[34m 152\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19903\u001b[0m | Best iteration: \u001b[1m\u001b[34m 188\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12967\u001b[0m | Best iteration: \u001b[1m\u001b[34m 191\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21836\u001b[0m | Best iteration: \u001b[1m\u001b[34m 159\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19840\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.26739\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 4, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.10696\u001b[0m | Best iteration: \u001b[1m\u001b[34m 489\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.32293\u001b[0m | Best iteration: \u001b[1m\u001b[34m 127\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21589\u001b[0m | Best iteration: \u001b[1m\u001b[34m 227\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.10011\u001b[0m | Best iteration: \u001b[1m\u001b[34m 554\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15223\u001b[0m | Best iteration: \u001b[1m\u001b[34m 214\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18039\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.16401\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 5, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13934\u001b[0m | Best iteration: \u001b[1m\u001b[34m 261\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14293\u001b[0m | Best iteration: \u001b[1m\u001b[34m 109\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.32063\u001b[0m | Best iteration: \u001b[1m\u001b[34m 120\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22011\u001b[0m | Best iteration: \u001b[1m\u001b[34m 206\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.29011\u001b[0m | Best iteration: \u001b[1m\u001b[34m 144\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.22243\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.14072\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Repeat \u001b[1m\u001b[34m#3\n",
      "Outer Loop fold 1, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19742\u001b[0m | Best iteration: \u001b[1m\u001b[34m 168\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15782\u001b[0m | Best iteration: \u001b[1m\u001b[34m 154\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16116\u001b[0m | Best iteration: \u001b[1m\u001b[34m 230\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.32973\u001b[0m | Best iteration: \u001b[1m\u001b[34m 195\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21102\u001b[0m | Best iteration: \u001b[1m\u001b[34m  97\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21088\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.26698\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 2, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19399\u001b[0m | Best iteration: \u001b[1m\u001b[34m 243\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13270\u001b[0m | Best iteration: \u001b[1m\u001b[34m 204\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19131\u001b[0m | Best iteration: \u001b[1m\u001b[34m 313\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15741\u001b[0m | Best iteration: \u001b[1m\u001b[34m 320\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.31282\u001b[0m | Best iteration: \u001b[1m\u001b[34m 162\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19770\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.11094\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 3, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.34358\u001b[0m | Best iteration: \u001b[1m\u001b[34m 144\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12833\u001b[0m | Best iteration: \u001b[1m\u001b[34m 213\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14617\u001b[0m | Best iteration: \u001b[1m\u001b[34m 252\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23438\u001b[0m | Best iteration: \u001b[1m\u001b[34m 132\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21326\u001b[0m | Best iteration: \u001b[1m\u001b[34m 201\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21372\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.24893\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 4, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17345\u001b[0m | Best iteration: \u001b[1m\u001b[34m 173\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21018\u001b[0m | Best iteration: \u001b[1m\u001b[34m 158\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13266\u001b[0m | Best iteration: \u001b[1m\u001b[34m 311\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.34382\u001b[0m | Best iteration: \u001b[1m\u001b[34m 118\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15295\u001b[0m | Best iteration: \u001b[1m\u001b[34m 276\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20288\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.12925\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 5, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.18063\u001b[0m | Best iteration: \u001b[1m\u001b[34m 123\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13392\u001b[0m | Best iteration: \u001b[1m\u001b[34m 250\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15180\u001b[0m | Best iteration: \u001b[1m\u001b[34m 289\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.26533\u001b[0m | Best iteration: \u001b[1m\u001b[34m 133\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.29773\u001b[0m | Best iteration: \u001b[1m\u001b[34m 121\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20588\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.24286\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Repeat \u001b[1m\u001b[34m#4\n",
      "Outer Loop fold 1, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13809\u001b[0m | Best iteration: \u001b[1m\u001b[34m 249\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.20680\u001b[0m | Best iteration: \u001b[1m\u001b[34m 152\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14719\u001b[0m | Best iteration: \u001b[1m\u001b[34m 201\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.06184\u001b[0m | Best iteration: \u001b[1m\u001b[34m 648\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22707\u001b[0m | Best iteration: \u001b[1m\u001b[34m 202\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.15598\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.28387\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 2, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22132\u001b[0m | Best iteration: \u001b[1m\u001b[34m 135\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24227\u001b[0m | Best iteration: \u001b[1m\u001b[34m 281\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.31741\u001b[0m | Best iteration: \u001b[1m\u001b[34m  95\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17668\u001b[0m | Best iteration: \u001b[1m\u001b[34m 162\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22062\u001b[0m | Best iteration: \u001b[1m\u001b[34m 296\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.23652\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.16221\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 3, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.05652\u001b[0m | Best iteration: \u001b[1m\u001b[34m 594\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22102\u001b[0m | Best iteration: \u001b[1m\u001b[34m 192\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.32180\u001b[0m | Best iteration: \u001b[1m\u001b[34m 141\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.20131\u001b[0m | Best iteration: \u001b[1m\u001b[34m 218\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.08270\u001b[0m | Best iteration: \u001b[1m\u001b[34m 362\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.17745\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20666\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 4, Inner Loop Training with \u001b[1m\u001b[34m494\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.10708\u001b[0m | Best iteration: \u001b[1m\u001b[34m 484\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.18123\u001b[0m | Best iteration: \u001b[1m\u001b[34m 289\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22164\u001b[0m | Best iteration: \u001b[1m\u001b[34m 132\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.40086\u001b[0m | Best iteration: \u001b[1m\u001b[34m  57\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12306\u001b[0m | Best iteration: \u001b[1m\u001b[34m 267\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20649\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.23201\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "Outer Loop fold 5, Inner Loop Training with \u001b[1m\u001b[34m493\u001b[0m samples, \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8602023\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.22620\u001b[0m | Best iteration: \u001b[1m\u001b[34m 185\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12869\u001b[0m | Best iteration: \u001b[1m\u001b[34m 236\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.27827\u001b[0m | Best iteration: \u001b[1m\u001b[34m 196\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24972\u001b[0m | Best iteration: \u001b[1m\u001b[34m 171\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.25074\u001b[0m | Best iteration: \u001b[1m\u001b[34m 241\u001b[0m\n",
      "\u001b[1m\u001b[31m Inner CV score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.22628\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.12366\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Inner CV avg score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20305\u001b[0m\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31m Outer Holdout avg score: \u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21093\u001b[0m\n",
      "**************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from shaphypetune import BoostBoruta\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 4\n",
    "    n_folds = 5\n",
    "    num_boost_round = 10000\n",
    "    seeds = [1, 42, 228, 265, 21, 8081988, 5062023, 666, 1488]\n",
    "\n",
    "params = {\n",
    "        'boosting_type':'goss',\n",
    "        'learning_rate': 0.06733232950390658, \n",
    "        'n_estimators': 50000, \n",
    "        'early_stopping_round' : 100, \n",
    "        'subsample' : 0.6970532011679706,\n",
    "        'colsample_bytree': 0.6055755840633003,\n",
    "        'num_leaves': 6,\n",
    "        'class_weight': 'balanced',\n",
    "        'metric': 'none', \n",
    "        'is_unbalance': True, \n",
    "        'random_state': 8062023,\n",
    "        'feature_fraction_seed': 8062023,\n",
    "        'bagging_seed': 8062023,\n",
    "        'max_depth': 8,\n",
    "        'reg_alpha': 0.08866046540248787,  \n",
    "        'reg_lambda': 1.0245261859148395e-06,\n",
    "        'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability  is replaced with max(min(,11015),1015)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1 - y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def calc_log_loss_weight(y_true): \n",
    "    '''w0, w1 assign different weights to individual data points during training.'''\n",
    "    nc = np.bincount(y_true)\n",
    "    w0, w1 = 1/(nc[0]/y_true.shape[0]), 1/(nc[1]/y_true.shape[0])\n",
    "    return w0, w1\n",
    "\n",
    "def lgbm_tuning(features, permut=False, boruta=False):\n",
    "    metric = balanced_log_loss\n",
    "    eval_results_ = {}\n",
    "\n",
    "    outer_cv_score = [] # store all cv scores of outer loop inference\n",
    "    inner_cv_score = [] # store all cv scores of inner loop training\n",
    "\n",
    "    perm_df_ = pd.DataFrame()\n",
    "    feature_importances_ = pd.DataFrame()\n",
    "    boruta_df_ = pd.DataFrame()\n",
    "    \n",
    "    for i in range(CFG.n_repeats):\n",
    "        print(f'Repeat {blu}#{i+1}')\n",
    "        \n",
    "        kf = MultilabelStratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=8062023+i)\n",
    "\n",
    "        # Stratify based on Class and Alpha (3 types of conditions)\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X=train_df[features], y=greeks.iloc[:,1:3]), start = 1): \n",
    "            X, y = train_df[features], train_df.Class\n",
    "#             X, y = generated_features_train, train_df.Class\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[train_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[train_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "            # 20% hold-out set\n",
    "            X_holdout, y_holdout = X_val, y_val\n",
    "\n",
    "            # Create an oof array for inner loop\n",
    "            oof_inner = np.zeros(len(X_train))\n",
    "\n",
    "            X_train = X_train.reset_index(drop=True)\n",
    "            y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "            cv = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=8062023+i) # Use stratifiedKfold to make life easier\n",
    "\n",
    "            X_outer, y_outer = X_train, y_train\n",
    "\n",
    "            models_ = [] # Used to store models trained in the inner loop.\n",
    "\n",
    "            print(f\"Outer Loop fold {fold}, Inner Loop Training with {blu}{X_train.shape[0]}{res} samples, {blu}{X_train.shape[1]}{res} features, seed = {blu}{8602023}{res}\")\n",
    "\n",
    "            for fold, (train_idx, val_idx) in enumerate(cv.split(X=X_train, y=y_train), start = 1):\n",
    "                # Split the dataset according to the fold indexes.\n",
    "                X_train = X_outer.iloc[train_idx]\n",
    "                X_val = X_outer.iloc[val_idx]\n",
    "                y_train = y_outer.iloc[train_idx]\n",
    "                y_val = y_outer.iloc[val_idx]\n",
    "\n",
    "    #             trn_w0, trn_w1 = calc_log_loss_weight(y_train)\n",
    "    #             val_w0, val_w1 = calc_log_loss_weight(y_train)\n",
    "\n",
    "    #             w_val = [y_train.map({0: trn_w0, 1: trn_w1}), y_val.map({0: val_w0, 1: val_w1})]\n",
    "\n",
    "                eval_results_[fold]= {}\n",
    "\n",
    "                clf = lgb.LGBMClassifier(**params)\n",
    "                clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                        eval_metric=bll_metric, # eval_sample_weight=w_val, \n",
    "                        early_stopping_rounds=300, verbose=-1)\n",
    "\n",
    "                models_.append(clf)\n",
    "\n",
    "                val_preds = clf.predict_proba(X_val)[:,1]\n",
    "                oof_inner[val_idx] = val_preds\n",
    "\n",
    "                val_score = metric(y_val, val_preds)\n",
    "                best_iter = clf.best_iteration_\n",
    "\n",
    "                print(f'Fold: {blu}{fold:>3}{res}| {metric.__name__}: {blu}{val_score:.5f}{res}'\n",
    "                      f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "                # permutation importance\n",
    "                if permut:\n",
    "                    perm = PermutationImportance(clf, scoring=None, n_iter=1, \n",
    "                                                 random_state=42, cv=None, refit=False).fit(X_val, y_val)\n",
    "\n",
    "                    perm_importance_df = pd.DataFrame({'importance': perm.feature_importances_}, \n",
    "                                                       index=X_val.columns).sort_index()\n",
    "\n",
    "                    if perm_df_.shape[0] == 0:\n",
    "                        perm_df_ = perm_importance_df.copy()\n",
    "                    else:\n",
    "                        perm_df_ += perm_importance_df\n",
    "\n",
    "                # tree feature importance\n",
    "                f_i = pd.DataFrame(sorted(zip(clf.feature_importances_, X.columns), \n",
    "                                                  reverse=True, key=lambda x: x[1]), \n",
    "                                   columns=['Value','Feature'])\n",
    "\n",
    "                if feature_importances_.shape[0] == 0:\n",
    "                    feature_importances_ = f_i.copy()\n",
    "                else:\n",
    "\n",
    "                    feature_importances_['Value'] += f_i['Value']\n",
    "                    \n",
    "                # Boruta SHAP importance\n",
    "                if boruta:\n",
    "                    model = BoostBoruta(clf, importance_type='shap_importances', train_importance=False)\n",
    "                    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                              eval_metric=bll_metric, early_stopping_rounds=300, verbose=-1)\n",
    "                    \n",
    "                    boruta_importance_df = pd.DataFrame({'importance': model.ranking_}, \n",
    "                                                         index=X_train.columns).sort_index()\n",
    "                    if boruta_df_.shape[0] == 0:\n",
    "                        boruta_df_ = boruta_importance_df.copy()\n",
    "                    else:\n",
    "                        boruta_df_ += boruta_importance_df\n",
    "\n",
    "            mean_cv_score = metric(y_outer, oof_inner)\n",
    "            print(f'{red} Inner CV score: {res} {metric.__name__}: {red}{mean_cv_score:.5f}{res}')\n",
    "            print(f'{\"*\" * 50}\\n')\n",
    "            inner_cv_score.append(mean_cv_score)\n",
    "\n",
    "            # infer holdout data using 5-fold model trained in inner loop\n",
    "            preds = np.zeros(len(X_holdout))\n",
    "            for model in models_:\n",
    "                preds += model.predict_proba(X_holdout)[:,1]\n",
    "            preds = preds / len(models_)\n",
    "            cv_score = metric(y_holdout, preds)\n",
    "            print(f'{red} Outer Holdout score: {res} {metric.__name__}: {red}{cv_score:.5f}{res}')\n",
    "            print(f'{\"*\" * 50}\\n')\n",
    "            outer_cv_score.append(cv_score)\n",
    "\n",
    "    print(f'{red} Inner CV avg score: {res} {metric.__name__}: {red}{np.mean(inner_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "\n",
    "    print(f'{red} Outer Holdout avg score: {res} {metric.__name__}: {red}{np.mean(outer_cv_score):.5f}{res}')\n",
    "    print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    if permut:\n",
    "        perm_df_ = perm_df_.sort_values('importance', ascending=False)\n",
    "        \n",
    "    if boruta:\n",
    "        boruta_df_ = boruta_df_.sort_values('importance')\n",
    "                                    \n",
    "    feature_importances_ = feature_importances_.sort_values('Value', ascending=False)\n",
    "    \n",
    "    return perm_df_, feature_importances_, boruta_df_, np.mean(inner_cv_score), np.mean(outer_cv_score)\n",
    "\n",
    "perm_df_, feature_importances_, boruta_df_, inner_cv_score, outer_cv_score = lgbm_tuning(features, permut=False, boruta=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminate features by their score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:11.610606Z",
     "iopub.status.busy": "2023-06-08T15:32:11.610256Z",
     "iopub.status.idle": "2023-06-08T15:32:11.617365Z",
     "shell.execute_reply": "2023-06-08T15:32:11.614040Z",
     "shell.execute_reply.started": "2023-06-08T15:32:11.610577Z"
    }
   },
   "outputs": [],
   "source": [
    "# f_dict = dict()\n",
    "# features = list(features)\n",
    "\n",
    "# for i, f in tqdm(enumerate(features), total=len(features)):\n",
    "#     _, __, inner_cv_score, outer_cv_score = lgbm_tuning(features[:i] + features[i+1:], permut=False)\n",
    "#     f_dict[f] = (inner_cv_score, outer_cv_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner CV avg score:  balanced_log_loss: 0.20305\n",
    "**************************************************\n",
    "\n",
    " Outer Holdout avg score:  balanced_log_loss: 0.21093\n",
    "**************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze permutation feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:11.619184Z",
     "iopub.status.busy": "2023-06-08T15:32:11.618864Z",
     "iopub.status.idle": "2023-06-08T15:32:11.652915Z",
     "shell.execute_reply": "2023-06-08T15:32:11.652069Z",
     "shell.execute_reply.started": "2023-06-08T15:32:11.619157Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# perm_df_.to_csv('perm_df.csv')\n",
    "# perm_df_\n",
    "# perm_cols = set(perm_df_.index[-20:])\n",
    "# perm_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze tree gain feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:11.654626Z",
     "iopub.status.busy": "2023-06-08T15:32:11.654102Z",
     "iopub.status.idle": "2023-06-08T15:32:11.669674Z",
     "shell.execute_reply": "2023-06-08T15:32:11.668840Z",
     "shell.execute_reply.started": "2023-06-08T15:32:11.654583Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature_importances_.to_csv('feature_importances.csv')\n",
    "# feature_importances_\n",
    "# fi_cols = set(feature_importances_['Feature'].values[-20:])\n",
    "# fi_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze BORUTA importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boruta_df_.to_csv('perm_df.csv')\n",
    "# boruta_df_\n",
    "# boruta_cols = set(boruta_df_.index[-20:])\n",
    "# boruta_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:11.672883Z",
     "iopub.status.busy": "2023-06-08T15:32:11.672350Z",
     "iopub.status.idle": "2023-06-08T15:32:38.349383Z",
     "shell.execute_reply": "2023-06-08T15:32:38.347823Z",
     "shell.execute_reply.started": "2023-06-08T15:32:11.672854Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m1\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.07147\u001b[0m | Best iteration: \u001b[1m\u001b[34m1083\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.29489\u001b[0m | Best iteration: \u001b[1m\u001b[34m 108\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15393\u001b[0m | Best iteration: \u001b[1m\u001b[34m 184\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24775\u001b[0m | Best iteration: \u001b[1m\u001b[34m 117\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14126\u001b[0m | Best iteration: \u001b[1m\u001b[34m 237\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18491\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m42\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30582\u001b[0m | Best iteration: \u001b[1m\u001b[34m 112\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14901\u001b[0m | Best iteration: \u001b[1m\u001b[34m 250\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.08629\u001b[0m | Best iteration: \u001b[1m\u001b[34m 504\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14131\u001b[0m | Best iteration: \u001b[1m\u001b[34m 354\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17714\u001b[0m | Best iteration: \u001b[1m\u001b[34m 176\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.17232\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m228\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14511\u001b[0m | Best iteration: \u001b[1m\u001b[34m 276\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23579\u001b[0m | Best iteration: \u001b[1m\u001b[34m 145\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.07179\u001b[0m | Best iteration: \u001b[1m\u001b[34m 451\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12002\u001b[0m | Best iteration: \u001b[1m\u001b[34m 302\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23517\u001b[0m | Best iteration: \u001b[1m\u001b[34m 165\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.15601\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m265\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.35232\u001b[0m | Best iteration: \u001b[1m\u001b[34m  95\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23679\u001b[0m | Best iteration: \u001b[1m\u001b[34m 175\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13571\u001b[0m | Best iteration: \u001b[1m\u001b[34m 337\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13105\u001b[0m | Best iteration: \u001b[1m\u001b[34m 219\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12203\u001b[0m | Best iteration: \u001b[1m\u001b[34m 302\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19526\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m21\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13438\u001b[0m | Best iteration: \u001b[1m\u001b[34m 160\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16390\u001b[0m | Best iteration: \u001b[1m\u001b[34m 179\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17699\u001b[0m | Best iteration: \u001b[1m\u001b[34m 247\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.39241\u001b[0m | Best iteration: \u001b[1m\u001b[34m 120\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16488\u001b[0m | Best iteration: \u001b[1m\u001b[34m 271\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18180\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8081988\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21092\u001b[0m | Best iteration: \u001b[1m\u001b[34m 156\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24488\u001b[0m | Best iteration: \u001b[1m\u001b[34m 154\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12813\u001b[0m | Best iteration: \u001b[1m\u001b[34m 307\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19543\u001b[0m | Best iteration: \u001b[1m\u001b[34m 222\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24656\u001b[0m | Best iteration: \u001b[1m\u001b[34m 142\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21001\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m5062023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19182\u001b[0m | Best iteration: \u001b[1m\u001b[34m 113\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.31431\u001b[0m | Best iteration: \u001b[1m\u001b[34m 200\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13467\u001b[0m | Best iteration: \u001b[1m\u001b[34m 194\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.25732\u001b[0m | Best iteration: \u001b[1m\u001b[34m  99\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.10656\u001b[0m | Best iteration: \u001b[1m\u001b[34m 214\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20010\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m666\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14874\u001b[0m | Best iteration: \u001b[1m\u001b[34m 302\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30158\u001b[0m | Best iteration: \u001b[1m\u001b[34m 119\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.28559\u001b[0m | Best iteration: \u001b[1m\u001b[34m 132\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.09771\u001b[0m | Best iteration: \u001b[1m\u001b[34m 368\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.07101\u001b[0m | Best iteration: \u001b[1m\u001b[34m 464\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18061\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m1488\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.11990\u001b[0m | Best iteration: \u001b[1m\u001b[34m 153\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19134\u001b[0m | Best iteration: \u001b[1m\u001b[34m 139\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21961\u001b[0m | Best iteration: \u001b[1m\u001b[34m 146\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23586\u001b[0m | Best iteration: \u001b[1m\u001b[34m 229\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.18710\u001b[0m | Best iteration: \u001b[1m\u001b[34m 281\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18773\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss for all seeds: \u001b[1m\u001b[31m0.18542\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    n_repeats = 4\n",
    "    n_folds = 5\n",
    "    num_boost_round = 10000\n",
    "    seeds = [1, 42, 228, 265, 21, 8081988, 5062023, 666, 1488]\n",
    "\n",
    "params = {\n",
    "        'boosting_type':'goss',\n",
    "        'learning_rate': 0.06733232950390658, \n",
    "        'n_estimators': 50000, \n",
    "        'early_stopping_round' : 100, \n",
    "        'subsample' : 0.6970532011679706,\n",
    "        'colsample_bytree': 0.6055755840633003,\n",
    "        'num_leaves': 6,\n",
    "        'class_weight': 'balanced',\n",
    "        'metric': 'none', \n",
    "        'is_unbalance': True, \n",
    "        'random_state': 8062023,\n",
    "        'feature_fraction_seed': 8062023,\n",
    "        'bagging_seed': 8062023,\n",
    "        'max_depth': 8,\n",
    "        'reg_alpha': 0.08866046540248787,  \n",
    "        'reg_lambda': 1.0245261859148395e-06,\n",
    "        'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability  is replaced with max(min(,11015),1015)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1 - y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def lgbm_training():\n",
    "    models_ = list()\n",
    "    all_cv_score = list()\n",
    "    weights_ = list()\n",
    "    \n",
    "    all_eval_results_ = dict() # used to store evaluation results for each seed\n",
    "    X, y = train_df[features], train_df.Class\n",
    "#     X, y = generated_features_train, train_df.Class\n",
    "    \n",
    "    for seed in CFG.seeds:   \n",
    "        kf = MultilabelStratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=seed)\n",
    "        metric = balanced_log_loss\n",
    "        eval_results_ = {}     # used to store evaluation results for each fold\n",
    "        oof = np.zeros(len(X))\n",
    "        print(f\"Training with {blu}{X.shape[1]}{res} features, seed = {blu}{seed}{res}\")\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(kf.split(X=train_df, y=greeks.iloc[:,1:3]), start = 1):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "#             trn_w0, trn_w1 = calc_log_loss_weight(y_train)\n",
    "#             val_w0, val_w1 = calc_log_loss_weight(y_val)\n",
    "\n",
    "#             w_val = [y_train.map({0: trn_w0, 1: trn_w1}), y_val.map({0: val_w0, 1: val_w1})]\n",
    "\n",
    "            clf = lgb.LGBMClassifier(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=bll_metric, early_stopping_rounds=300,\n",
    "                    verbose=-1)\n",
    "\n",
    "            eval_results_[fold]= clf.evals_result_\n",
    "            \n",
    "            val_preds = clf.predict_proba(X_val)[:,1]\n",
    "            oof[val_idx] = val_preds\n",
    "\n",
    "            val_score = balanced_log_loss(y_val, val_preds)\n",
    "            best_iter = clf.best_iteration_\n",
    "            \n",
    "            print(f'Fold: {blu}{fold:>3}{res}| {metric.__name__}: {blu}{val_score:.5f}{res}'\n",
    "                  f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # Stores the model\n",
    "            models_.append(clf)\n",
    "            \n",
    "            # Stores evaluation result\n",
    "            all_eval_results_[seed] = eval_results_\n",
    "            \n",
    "            # Store prediction weight\n",
    "            weights_.append(1/clf.evals_result_['valid_1']['balanced_log_loss'][clf.best_iteration_])\n",
    "            \n",
    "#         oof[(oof > 0.8) & (oof < 0.9)] = 0.9\n",
    "#         oof[(oof < 0.2) & (oof > 0.1)] = 0.1\n",
    "        mean_cv_score = metric(y, oof)\n",
    "        \n",
    "        \n",
    "        all_cv_score.append(mean_cv_score)\n",
    "        print(f'{red}Mean{res} {metric.__name__}: {red}{mean_cv_score:.5f}')\n",
    "        print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    print(f'{red}Mean{res} {metric.__name__} for all seeds: {red}{np.mean(all_cv_score):.5f}')\n",
    "    return models_, eval_results_, all_eval_results_, weights_\n",
    "\n",
    "models_, eval_results_, all_eval_results_, weights_ = lgbm_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean balanced_log_loss for all seeds: 0.18615"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T15:32:38.350522Z",
     "iopub.status.idle": "2023-06-08T15:32:38.351460Z",
     "shell.execute_reply": "2023-06-08T15:32:38.351239Z",
     "shell.execute_reply.started": "2023-06-08T15:32:38.351217Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Model Evaluation\n",
    "# metric_score_folds = pd.DataFrame.from_dict(all_eval_results_)\n",
    "# fit_logloss = []\n",
    "# val_logloss = []\n",
    "\n",
    "# for seed in CFG.seeds:\n",
    "#     for fold in range(1,CFG.n_folds+1):\n",
    "#         fit_logloss.append(metric_score_folds[seed][fold]['training']['balanced_log_loss'])\n",
    "#         val_logloss.append(metric_score_folds[seed][fold]['valid_1']['balanced_log_loss'])\n",
    "\n",
    "# fig, axes = plt.subplots(math.ceil(CFG.n_folds*len(CFG.seeds)/CFG.n_folds), CFG.n_folds, figsize=(20, 20), dpi=150)\n",
    "# ax = axes.flatten()\n",
    "# for i, (f, v, m) in enumerate(zip(fit_logloss, val_logloss, models_), start = 1): \n",
    "#     sns.lineplot(f, color='#B90000', ax=ax[i-1], label='fit')\n",
    "#     sns.lineplot(v, color='#048BA8', ax=ax[i-1], label='val')\n",
    "#     ax[i-1].legend()\n",
    "#     ax[i-1].spines['top'].set_visible(False);\n",
    "#     ax[i-1].spines['right'].set_visible(False)\n",
    "#     ax[i-1].set_title(f'Seed {CFG.seeds[(i-1)//CFG.n_folds]} Fold {CFG.n_folds if i%CFG.n_folds==0 else i%CFG.n_folds}', fontdict={'fontweight': 'bold'})\n",
    "\n",
    "#     color =  ['#048BA8', palette[-3]]\n",
    "#     best_iter = m.best_iteration_\n",
    "#     span_range = [[0, best_iter], [best_iter + 10, best_iter + CFG.num_boost_round]]\n",
    "\n",
    "#     for idx, sub_title in enumerate([f'Best\\nIteration: {best_iter}', f'Early\\n Stopping: 2000']):\n",
    "#         ax[i-1].annotate(sub_title,\n",
    "#                     xy=(sum(span_range[idx])/2 , 0.5),\n",
    "#                     xytext=(0,0), textcoords='offset points',\n",
    "#                     va=\"center\", ha=\"center\",\n",
    "#                     color=\"w\", fontsize=16, fontweight='bold',\n",
    "#                     bbox=dict(boxstyle='round4', pad=0.4, color=color[idx], alpha=0.6))\n",
    "#         ax[i-1].axvspan(span_range[idx][0]-0.4,span_range[idx][1]+0.4,  color=color[idx], alpha=0.07)\n",
    "\n",
    "#     ax[i-1].set_xlim(0, best_iter + 20 + 2000)\n",
    "#     ax[i-1].legend(bbox_to_anchor=(0.95, 1), loc='upper right', title='logloss')\n",
    "\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T15:32:38.352845Z",
     "iopub.status.idle": "2023-06-08T15:32:38.353222Z",
     "shell.execute_reply": "2023-06-08T15:32:38.353059Z",
     "shell.execute_reply.started": "2023-06-08T15:32:38.353042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id   class_0   class_1\n",
       "0  00eed32682bb  0.590987  0.409013\n",
       "1  010ebe33f668  0.590987  0.409013\n",
       "2  02fa521e1838  0.590987  0.409013\n",
       "3  040e15f562a2  0.590987  0.409013\n",
       "4  046e85c7cc7f  0.590987  0.409013"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_preds = np.zeros((test_df.shape[0],2))\n",
    "\n",
    "# for i in range(len(final_test_predictions)):\n",
    "#     test_preds[:, 0] += test_weights[i] * final_test_predictions[i][:, 0]\n",
    "#     test_preds[:, 1] += test_weights[i] * final_test_predictions[i][:, 1]\n",
    "\n",
    "# test_preds /= sum(test_weights)\n",
    "\n",
    "def predict(X):\n",
    "    y = np.zeros(len(X))\n",
    "    for i, model in enumerate(models_):\n",
    "#         y += weights_[i] * model.predict_proba(X)[:,1]\n",
    "        y += model.predict_proba(X)[:,1]\n",
    "#     return y / sum(weights_)\n",
    "    return y / len(models_)\n",
    "\n",
    "predictions = predict(test_df[features])\n",
    "# predictions = predict(generated_features_test)\n",
    "\n",
    "test_df['class_1'] = predictions\n",
    "test_df['class_0'] = 1 - predictions\n",
    "\n",
    "sample_submission[['class_0', 'class_1']] = test_df[['class_0', 'class_1']]\n",
    "sample_submission.to_csv(r\"submission.csv\", index=False)\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a lot of resulting features. I have already identified a few important once. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
