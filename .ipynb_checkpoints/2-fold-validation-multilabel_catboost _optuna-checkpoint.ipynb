{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:23.665663Z",
     "iopub.status.busy": "2023-06-08T15:31:23.665223Z",
     "iopub.status.idle": "2023-06-08T15:31:35.997437Z",
     "shell.execute_reply": "2023-06-08T15:31:35.996163Z",
     "shell.execute_reply.started": "2023-06-08T15:31:23.665630Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import catboost as cat\n",
    "from catboost import Pool\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, BaseShuffleSplit, _validate_shuffle_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.validation import _num_samples, check_array\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "import eli5\n",
    "from IPython.display import display\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from colorama import Style, Fore\n",
    "\n",
    "palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n",
    "           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n",
    "\n",
    "blk = Style.BRIGHT + Fore.BLACK\n",
    "red = Style.BRIGHT + Fore.RED\n",
    "blu = Style.BRIGHT + Fore.BLUE\n",
    "res = Style.RESET_ALL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:36.001085Z",
     "iopub.status.busy": "2023-06-08T15:31:35.999808Z",
     "iopub.status.idle": "2023-06-08T15:31:36.066222Z",
     "shell.execute_reply": "2023-06-08T15:31:36.064862Z",
     "shell.execute_reply.started": "2023-06-08T15:31:36.001050Z"
    }
   },
   "outputs": [],
   "source": [
    "# COMP_PATH = \"/kaggle/input/icr-identify-age-related-conditions\"\n",
    "COMP_PATH = \"icr-identify-age-related-conditions\"\n",
    "\n",
    "train_df = pd.read_csv(f'{COMP_PATH}//train.csv')\n",
    "test_df = pd.read_csv(f'{COMP_PATH}/test.csv')\n",
    "greeks = pd.read_csv(f\"{COMP_PATH}/greeks.csv\")\n",
    "sample_submission = pd.read_csv(f\"{COMP_PATH}/sample_submission.csv\")\n",
    "\n",
    "train_df['EJ'] = train_df['EJ'].replace({'A': 0, 'B': 1})\n",
    "test_df['EJ'] = test_df['EJ'].replace({'A': 0, 'B': 1})\n",
    "\n",
    "train_df.columns = train_df.columns.str.replace(' ', '')\n",
    "test_df.columns = test_df.columns.str.replace(' ', '')\n",
    "\n",
    "# train_df.drop('Id',axis=1, inplace=True)\n",
    "# train_df.fillna(train_df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:36.069293Z",
     "iopub.status.busy": "2023-06-08T15:31:36.068506Z",
     "iopub.status.idle": "2023-06-08T15:31:36.075275Z",
     "shell.execute_reply": "2023-06-08T15:31:36.074052Z",
     "shell.execute_reply.started": "2023-06-08T15:31:36.069253Z"
    }
   },
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# new_num_cols = train_df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# train_df[new_num_cols] = scaler.fit_transform(train_df[new_num_cols])\n",
    "# test_df[new_num_cols] = scaler.transform(test_df[new_num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine features in all possible ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi = pd.read_csv('feature_importances.csv', index_col = 'Unnamed: 0')\n",
    "# fi_cols = set(fi['Feature'].head(100).values)\n",
    "\n",
    "# perm = pd.read_csv('perm_df.csv', index_col = 'Unnamed: 0')\n",
    "# perm_cols = set(perm['importance'].head(100).index)\n",
    "\n",
    "# important_col = list(perm_cols.intersection(fi_cols))\n",
    "# print(important_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [fe for fe in train_df.columns if fe not in ['Id','CF', 'CB', 'DV', 'BR', 'DF', 'AR', 'GI', 'AY', 'GB',\n",
    "#                                                         'AH', 'CW', 'CL', 'Class', 'BP']]\n",
    "\n",
    "# for f in features:\n",
    "#     train_df[f] = np.floor(train_df[f]*1000)/1000 # quality decreases no significant result for LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log features (preserve sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for f in features:\n",
    "#     train_df[f] = np.sign(train_df[f]) * np.log1p(np.abs(train_df[f])) # no significant result for LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:36.080972Z",
     "iopub.status.busy": "2023-06-08T15:31:36.079281Z",
     "iopub.status.idle": "2023-06-08T15:31:44.412385Z",
     "shell.execute_reply": "2023-06-08T15:31:44.411508Z",
     "shell.execute_reply.started": "2023-06-08T15:31:36.080912Z"
    }
   },
   "outputs": [],
   "source": [
    "# features = train_df.drop(['Class', 'Id'], axis=1).columns\n",
    "\n",
    "features = [fe for fe in train_df.columns if fe not in ['Id','Class']]\n",
    "\n",
    "# features = [fe for fe in train_df.columns if fe not in ['Id','CF', 'CB', 'DV', 'BR', 'DF', 'AR', 'GI', 'AY', 'GB',\n",
    "#                                                         'AH', 'CW', 'CL', 'Class', 'BP', 'AX', 'AZ', 'BD', 'BZ', 'EG', \n",
    "#                                                         'EH', 'EJ', 'FC', 'FD', 'FE', 'FS', 'GF', 'GH']]\n",
    "\n",
    "# ['AX', 'AZ', 'BD', 'BZ', 'EG', 'EH', 'EJ', 'FC', 'FD', 'FE', 'FS', 'GF', 'GH']\n",
    "\n",
    "# selected = ['CR+DU', 'EH-FI', 'CD-DL', 'AB-FI', 'FL+GL', 'DU-EP', 'BQ+EP', 'FR+GL', 'CD-EP', 'AF-EG', \n",
    "#             'CU-DU', 'AB-CR', 'BQ+CD', 'BQ-GF', 'BQ+DI', 'CR+DH', 'CD+EL', 'CC+EH', 'DA-DU', 'AF+BQ', \n",
    "#             'CR+EH', 'BQ-DN', 'DN-EL', 'EJ+FI', 'BQ-EP', 'EE*EP', 'BQ/CU', 'AB*FR', 'EJ/FL', 'CD/CH', \n",
    "#             'CD/EP', 'CR*DU', 'AF/EG', 'CC/CD', 'AB/CH', 'BQ/CH', 'CH/DI', 'AB/DN', 'DU/EP', 'DU/EJ', \n",
    "#             'DU/GL', 'DY/EE', 'CR*CS', 'DH/DU', 'EJ*GL', 'CD/DL', 'DH/DI', 'DU*FR', 'CR*DH']\n",
    "\n",
    "# def gen_features(features, df):\n",
    "#     generated_features = pd.DataFrame()\n",
    "\n",
    "#     for fe_a, fe_b in tqdm(itertools.combinations(features, 2), total=sum([1 for i in itertools.combinations(features, 2)])):\n",
    "#         generated_features[f'{fe_a}+{fe_b}']   = df[fe_a] + df[fe_b]\n",
    "#         generated_features[f'{fe_a}-{fe_b}']   = df[fe_a] - df[fe_b] \n",
    "#         generated_features[f'{fe_a}*{fe_b}']   = df[fe_a] * df[fe_b]\n",
    "#         generated_features[f'{fe_a}/{fe_b}']   = df[fe_a] / df[fe_b]\n",
    "\n",
    "# #         generated_features[f'{fe_a}_2']        = df[fe_a].pow(2)\n",
    "# #         generated_features[f'{fe_b}_2']        = df[fe_b].pow(2)\n",
    "# #         generated_features[f'{fe_a}*{fe_b}_2'] = df[fe_a] * df[fe_b].pow(2)\n",
    "# #         generated_features[f'{fe_a}_2*{fe_b}'] = df[fe_a].pow(2) * df[fe_b]\n",
    "\n",
    "# #         generated_features[f'{fe_a}_05'] = df[fe_a].pow(0.5)\n",
    "# #         generated_features[f'{fe_b}_05'] = df[fe_b].pow(0.5)\n",
    "# #         generated_features[f'{fe_a}*{fe_b}_05'] = df[fe_a] * df[fe_b].pow(0.5)\n",
    "# #         generated_features[f'{fe_a}_05*{fe_b}'] = df[fe_a].pow(0.5) * df[fe_b]\n",
    "\n",
    "# #         generated_features[f'{fe_a}_log'] = np.log(df[fe_a])\n",
    "# #         generated_features[f'{fe_b}_log'] = np.log(df[fe_b])\n",
    "# #         generated_features[f'{fe_a}*{fe_b}_log'] = df[fe_a] * np.log(df[fe_b])\n",
    "# #         generated_features[f'{fe_a}_log*{fe_b}'] = np.log(df[fe_a]) * df[fe_b]\n",
    "        \n",
    "#     generated_features = generated_features[selected]\n",
    "#     generated_features = pd.concat([generated_features, df[features]], axis=1)\n",
    "    \n",
    "#     # prevent inf\n",
    "#     for g in generated_features.columns:\n",
    "#         generated_features[g] = np.minimum(np.maximum(generated_features[g], -1e9), 1e9)\n",
    "    \n",
    "#     return generated_features\n",
    "\n",
    "# generated_features_train = gen_features(features, train_df)\n",
    "# generated_features_test = gen_features(features, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:31:44.414098Z",
     "iopub.status.busy": "2023-06-08T15:31:44.413784Z",
     "iopub.status.idle": "2023-06-08T15:31:44.461531Z",
     "shell.execute_reply": "2023-06-08T15:31:44.460184Z",
     "shell.execute_reply.started": "2023-06-08T15:31:44.414071Z"
    }
   },
   "outputs": [],
   "source": [
    "def IterativeStratification(labels, r, random_state):\n",
    "    \"\"\"This function implements the Iterative Stratification algorithm described\n",
    "    in the following paper:\n",
    "    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n",
    "    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n",
    "    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n",
    "    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n",
    "    Heidelberg.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = labels.shape[0]\n",
    "    test_folds = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    # Calculate the desired number of examples at each subset\n",
    "    c_folds = r * n_samples\n",
    "\n",
    "    # Calculate the desired number of examples of each label at each subset\n",
    "    c_folds_labels = np.outer(r, labels.sum(axis=0))\n",
    "\n",
    "    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n",
    "\n",
    "    while np.any(labels_not_processed_mask):\n",
    "        # Find the label with the fewest (but at least one) remaining examples,\n",
    "        # breaking ties randomly\n",
    "        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n",
    "\n",
    "        # Handle case where only all-zero labels are left by distributing\n",
    "        # across all folds as evenly as possible (not in original algorithm but\n",
    "        # mentioned in the text). (By handling this case separately, some\n",
    "        # code redundancy is introduced; however, this approach allows for\n",
    "        # decreased execution time when there are a relatively large number\n",
    "        # of all-zero labels.)\n",
    "        if num_labels.sum() == 0:\n",
    "            sample_idxs = np.where(labels_not_processed_mask)[0]\n",
    "\n",
    "            for sample_idx in sample_idxs:\n",
    "                fold_idx = np.where(c_folds == c_folds.max())[0]\n",
    "\n",
    "                if fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n",
    "\n",
    "                test_folds[sample_idx] = fold_idx\n",
    "                c_folds[fold_idx] -= 1\n",
    "\n",
    "            break\n",
    "\n",
    "        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n",
    "        if label_idx.shape[0] > 1:\n",
    "            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n",
    "\n",
    "        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n",
    "\n",
    "        for sample_idx in sample_idxs:\n",
    "            # Find the subset(s) with the largest number of desired examples\n",
    "            # for this label, breaking ties by considering the largest number\n",
    "            # of desired examples, breaking further ties randomly\n",
    "            label_folds = c_folds_labels[:, label_idx]\n",
    "            fold_idx = np.where(label_folds == label_folds.max())[0]\n",
    "\n",
    "            if fold_idx.shape[0] > 1:\n",
    "                temp_fold_idx = np.where(c_folds[fold_idx] ==\n",
    "                                         c_folds[fold_idx].max())[0]\n",
    "                fold_idx = fold_idx[temp_fold_idx]\n",
    "\n",
    "                if temp_fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n",
    "\n",
    "            test_folds[sample_idx] = fold_idx\n",
    "            labels_not_processed_mask[sample_idx] = False\n",
    "\n",
    "            # Update desired number of examples\n",
    "            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n",
    "            c_folds[fold_idx] -= 1\n",
    "\n",
    "    return test_folds\n",
    "\n",
    "\n",
    "class MultilabelStratifiedKFold(_BaseKFold):\n",
    "    \"\"\"Multilabel stratified K-Folds cross-validator\n",
    "    Provides train/test indices to split multilabel data into train/test sets.\n",
    "    This cross-validation object is a variation of KFold that returns\n",
    "    stratified folds for multilabel data. The folds are made by preserving\n",
    "    the percentage of samples for each label.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=3\n",
    "        Number of folds. Must be at least 2.\n",
    "    shuffle : boolean, optional\n",
    "        Whether to shuffle each stratification of the data before splitting\n",
    "        into batches.\n",
    "    random_state : int, RandomState instance or None, optional, default=None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedKFold that only uses random_state\n",
    "        when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n",
    "    >>> mskf.get_n_splits(X, y)\n",
    "    2\n",
    "    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n",
    "    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n",
    "    >>> for train_index, test_index in mskf.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different in each fold.\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=3, *, shuffle=False, random_state=None):\n",
    "        super(MultilabelStratifiedKFold, self).__init__(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "\n",
    "    def _make_test_folds(self, X, y):\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n",
    "\n",
    "        num_samples = y.shape[0]\n",
    "\n",
    "        rng = check_random_state(self.random_state)\n",
    "        indices = np.arange(num_samples)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(indices)\n",
    "            y = y[indices]\n",
    "\n",
    "        r = np.asarray([1 / self.n_splits] * self.n_splits)\n",
    "\n",
    "        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "        return test_folds[np.argsort(indices)]\n",
    "\n",
    "    def _iter_test_masks(self, X=None, y=None, groups=None):\n",
    "        test_folds = self._make_test_folds(X, y)\n",
    "        for i in range(self.n_splits):\n",
    "            yield test_folds == i\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n",
    "\n",
    "\n",
    "class RepeatedMultilabelStratifiedKFold(_RepeatedSplits):\n",
    "    \"\"\"Repeated Multilabel Stratified K-Fold cross validator.\n",
    "    Repeats Mulilabel Stratified K-Fold n times with different randomization\n",
    "    in each repetition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of folds. Must be at least 2.\n",
    "    n_repeats : int, default=10\n",
    "        Number of times cross-validator needs to be repeated.\n",
    "    random_state : None, int or RandomState, default=None\n",
    "        Random state to be used to generate random state for each\n",
    "        repetition as well as randomly breaking ties within the iterative\n",
    "        stratification algorithm.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> rmskf = RepeatedMultilabelStratifiedKFold(n_splits=2, n_repeats=2,\n",
    "    ...     random_state=0)\n",
    "    >>> for train_index, test_index in rmskf.split(X, y):\n",
    "    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...     X_train, X_test = X[train_index], X[test_index]\n",
    "    ...     y_train, y_test = y[train_index], y[test_index]\n",
    "    ...\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [0 1 4 5] TEST: [2 3 6 7]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedStratifiedKFold: Repeats (Non-multilabel) Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, *, n_repeats=10, random_state=None):\n",
    "        super(RepeatedMultilabelStratifiedKFold, self).__init__(\n",
    "            MultilabelStratifiedKFold, n_repeats=n_repeats, random_state=random_state,\n",
    "            n_splits=n_splits)\n",
    "\n",
    "\n",
    "class MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n",
    "    \"\"\"Multilabel Stratified ShuffleSplit cross-validator\n",
    "    Provides train/test indices to split data into train/test sets.\n",
    "    This cross-validation object is a merge of MultilabelStratifiedKFold and\n",
    "    ShuffleSplit, which returns stratified randomized folds for multilabel\n",
    "    data. The folds are made by preserving the percentage of each label.\n",
    "    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n",
    "    do not guarantee that all folds will be different, although this is\n",
    "    still very likely for sizeable datasets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default 10\n",
    "        Number of re-shuffling & splitting iterations.\n",
    "    test_size : float, int, None, optional\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split. If int, represents the\n",
    "        absolute number of test samples. If None, the value is set to the\n",
    "        complement of the train size. By default, the value is set to 0.1.\n",
    "        The default will change in version 0.21. It will remain 0.1 only\n",
    "        if ``train_size`` is unspecified, otherwise it will complement\n",
    "        the specified ``train_size``.\n",
    "    train_size : float, int, or None, default is None\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedShuffleSplit that only uses\n",
    "        random_state when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n",
    "    ...    random_state=0)\n",
    "    >>> msss.get_n_splits(X, y)\n",
    "    3\n",
    "    >>> print(mss)       # doctest: +ELLIPSIS\n",
    "    MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n",
    "                                     train_size=None)\n",
    "    >>> for train_index, test_index in msss.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different from desired due to the\n",
    "    preference of stratification over perfectly sized folds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=10, *, test_size=\"default\", train_size=None,\n",
    "                 random_state=None):\n",
    "        super(MultilabelStratifiedShuffleSplit, self).__init__(\n",
    "            n_splits=n_splits, test_size=test_size, train_size=train_size, random_state=random_state)\n",
    "\n",
    "    def _iter_indices(self, X, y, groups=None):\n",
    "        n_samples = _num_samples(X)\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n",
    "                    type_of_target_y))\n",
    "\n",
    "        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n",
    "                                                  self.train_size)\n",
    "\n",
    "        n_samples = y.shape[0]\n",
    "        rng = check_random_state(self.random_state)\n",
    "        y_orig = y.copy()\n",
    "\n",
    "        r = np.array([n_train, n_test]) / (n_train + n_test)\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            indices = np.arange(n_samples)\n",
    "            rng.shuffle(indices)\n",
    "            y = y_orig[indices]\n",
    "\n",
    "            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "            test_idx = test_folds[np.argsort(indices)] == 1\n",
    "            test = np.where(test_idx)[0]\n",
    "            train = np.where(~test_idx)[0]\n",
    "\n",
    "            yield train, test\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Optuna optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "class CFG:\n",
    "    n_repeats = 2\n",
    "    n_folds = 2\n",
    "    num_boost_round = 10000\n",
    "    seeds = [1, 42, 228, 265, 21, 8081988, 5062023, 666, 1488]\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "    \n",
    "    # In order to avoid the extremes of the log function, each predicted probability ð‘ is replaced with max(min(ð‘,1âˆ’10âˆ’15),10âˆ’15)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1 - y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def catboost_opt(features, n_trials):\n",
    "    X, y = train_df[features], train_df.Class\n",
    "    \n",
    "    def objective(trial):\n",
    "        bll_list = list()\n",
    "        \n",
    "        for i in range(CFG.n_repeats):\n",
    "            print(f'Repeat {blu}#{i+1}')\n",
    "\n",
    "            # Create an oof array for inner loop\n",
    "            oof = np.zeros(train_df.shape[0])\n",
    "\n",
    "            kf = MultilabelStratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=8062023+i)\n",
    "\n",
    "            # Stratify based on Class and Alpha (3 types of conditions)\n",
    "            for fold, (train_idx, val_idx) in enumerate(kf.split(X=train_df[features], y=greeks.iloc[:,1:3]), start = 1): \n",
    "\n",
    "                # Split the dataset according to the fold indexes.\n",
    "                X_train = X.iloc[train_idx]\n",
    "                X_val = X.iloc[val_idx]\n",
    "                y_train = y.iloc[train_idx]\n",
    "                y_val = y.iloc[val_idx]\n",
    "          \n",
    "                train_pool = Pool(X_train, y_train, cat_features=['EJ'])\n",
    "                val_pool = Pool(X_val, y_val, cat_features=['EJ'])\n",
    "\n",
    "                # Parameters\n",
    "                params = {\n",
    "                    'auto_class_weights': 'Balanced',\n",
    "                    'task_type': 'GPU',\n",
    "                    'eval_metric': 'Logloss',\n",
    "                    'loss_function': 'Logloss', \n",
    "                    'random_seed': 10062023,\n",
    "                    'od_type': 'Iter', # Type of overfitting detector - stop after k iteraions\n",
    "                    'od_wait': 30, # Overfitting detector - stop training after k iterations without metric improvement\n",
    "                    'metric_period': 100, # Show metric each k iterations\n",
    "                    'iterations' : trial.suggest_int('iterations', 300, 1200),                         \n",
    "                    'l2_leaf_reg': trial.suggest_loguniform(\"l2_leaf_reg\", 1e-8, 100),\n",
    "                    'learning_rate' :trial.suggest_loguniform('learning_rate', 1e-3, 3e-1),             \n",
    "                    'grow_policy':trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']), \n",
    "                    'depth' : trial.suggest_int('depth', 4, 10),  # Max tree depth\n",
    "                    'random_strength' :trial.suggest_int('random_strength', 0, 100), # The amount of randomness to use \n",
    "                                                                                     # for scoring splits when the tree structure\n",
    "                                                                                     # is selected. Helps to avoid overfitting\n",
    "                    'max_bin': trial.suggest_categorical('max_bin', [2,3,4,5,10,20,32,64]), # The number of splits for \n",
    "                                                                                            # numerical features\n",
    "\n",
    "                    'bagging_temperature' : trial.suggest_loguniform('bagging_temperature', 0.01, 100.00), # Assigns random \n",
    "                                                                                                           # weights to objects\n",
    "                }\n",
    "\n",
    "                if params['grow_policy'] == 'SymmetricTree': \n",
    "                    params['boosting_type']= trial.suggest_categorical('boosting_type', ['Ordered', 'Plain'])\n",
    "                else:\n",
    "                    params['boosting_type'] = 'Plain'\n",
    "\n",
    "                # Learning\n",
    "                model = cat.CatBoostClassifier(**params)     \n",
    "                model.fit(train_pool, eval_set=val_pool)\n",
    "                # Predict\n",
    "                preds = model.predict_proba(val_pool)[:,1]\n",
    "                # Evaluation\n",
    "                bll = balanced_log_loss(y_val, preds)\n",
    "                bll_list.append(bll)\n",
    "                \n",
    "        return np.mean(bll_list)\n",
    "            \n",
    "    study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    df = study.trials_dataframe()\n",
    "    df.sort_values('value').iloc[:, [1] + list(range(5, 14))]\n",
    "    df.to_csv(f'optuna_catboost_fold_.csv')\n",
    "            \n",
    "catboost_opt(features, n_trials=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-08T15:32:11.672883Z",
     "iopub.status.busy": "2023-06-08T15:32:11.672350Z",
     "iopub.status.idle": "2023-06-08T15:32:38.349383Z",
     "shell.execute_reply": "2023-06-08T15:32:38.347823Z",
     "shell.execute_reply.started": "2023-06-08T15:32:11.672854Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m1\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.07147\u001b[0m | Best iteration: \u001b[1m\u001b[34m1083\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.29489\u001b[0m | Best iteration: \u001b[1m\u001b[34m 108\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.15393\u001b[0m | Best iteration: \u001b[1m\u001b[34m 184\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24775\u001b[0m | Best iteration: \u001b[1m\u001b[34m 117\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14126\u001b[0m | Best iteration: \u001b[1m\u001b[34m 237\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18491\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m42\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30582\u001b[0m | Best iteration: \u001b[1m\u001b[34m 112\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14901\u001b[0m | Best iteration: \u001b[1m\u001b[34m 250\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.08629\u001b[0m | Best iteration: \u001b[1m\u001b[34m 504\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14131\u001b[0m | Best iteration: \u001b[1m\u001b[34m 354\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17714\u001b[0m | Best iteration: \u001b[1m\u001b[34m 176\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.17232\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m228\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14511\u001b[0m | Best iteration: \u001b[1m\u001b[34m 276\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23579\u001b[0m | Best iteration: \u001b[1m\u001b[34m 145\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.07179\u001b[0m | Best iteration: \u001b[1m\u001b[34m 451\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12002\u001b[0m | Best iteration: \u001b[1m\u001b[34m 302\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23517\u001b[0m | Best iteration: \u001b[1m\u001b[34m 165\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.15601\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m265\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.35232\u001b[0m | Best iteration: \u001b[1m\u001b[34m  95\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23679\u001b[0m | Best iteration: \u001b[1m\u001b[34m 175\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13571\u001b[0m | Best iteration: \u001b[1m\u001b[34m 337\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13105\u001b[0m | Best iteration: \u001b[1m\u001b[34m 219\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12203\u001b[0m | Best iteration: \u001b[1m\u001b[34m 302\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.19526\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m21\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13438\u001b[0m | Best iteration: \u001b[1m\u001b[34m 160\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16390\u001b[0m | Best iteration: \u001b[1m\u001b[34m 179\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.17699\u001b[0m | Best iteration: \u001b[1m\u001b[34m 247\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.39241\u001b[0m | Best iteration: \u001b[1m\u001b[34m 120\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.16488\u001b[0m | Best iteration: \u001b[1m\u001b[34m 271\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18180\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m8081988\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21092\u001b[0m | Best iteration: \u001b[1m\u001b[34m 156\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24488\u001b[0m | Best iteration: \u001b[1m\u001b[34m 154\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.12813\u001b[0m | Best iteration: \u001b[1m\u001b[34m 307\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19543\u001b[0m | Best iteration: \u001b[1m\u001b[34m 222\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.24656\u001b[0m | Best iteration: \u001b[1m\u001b[34m 142\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.21001\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m5062023\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19182\u001b[0m | Best iteration: \u001b[1m\u001b[34m 113\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.31431\u001b[0m | Best iteration: \u001b[1m\u001b[34m 200\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.13467\u001b[0m | Best iteration: \u001b[1m\u001b[34m 194\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.25732\u001b[0m | Best iteration: \u001b[1m\u001b[34m  99\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.10656\u001b[0m | Best iteration: \u001b[1m\u001b[34m 214\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.20010\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m666\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.14874\u001b[0m | Best iteration: \u001b[1m\u001b[34m 302\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.30158\u001b[0m | Best iteration: \u001b[1m\u001b[34m 119\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.28559\u001b[0m | Best iteration: \u001b[1m\u001b[34m 132\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.09771\u001b[0m | Best iteration: \u001b[1m\u001b[34m 368\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.07101\u001b[0m | Best iteration: \u001b[1m\u001b[34m 464\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18061\n",
      "**************************************************\n",
      "\n",
      "Training with \u001b[1m\u001b[34m30\u001b[0m features, seed = \u001b[1m\u001b[34m1488\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  1\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.11990\u001b[0m | Best iteration: \u001b[1m\u001b[34m 153\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  2\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.19134\u001b[0m | Best iteration: \u001b[1m\u001b[34m 139\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  3\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.21961\u001b[0m | Best iteration: \u001b[1m\u001b[34m 146\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  4\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.23586\u001b[0m | Best iteration: \u001b[1m\u001b[34m 229\u001b[0m\n",
      "Fold: \u001b[1m\u001b[34m  5\u001b[0m| balanced_log_loss: \u001b[1m\u001b[34m0.18710\u001b[0m | Best iteration: \u001b[1m\u001b[34m 281\u001b[0m\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss: \u001b[1m\u001b[31m0.18773\n",
      "**************************************************\n",
      "\n",
      "\u001b[1m\u001b[31mMean\u001b[0m balanced_log_loss for all seeds: \u001b[1m\u001b[31m0.18542\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    n_repeats = 4\n",
    "    n_folds = 5\n",
    "    num_boost_round = 10000\n",
    "    seeds = [1, 42, 228, 265, 21, 8081988, 5062023, 666, 1488]\n",
    "\n",
    "params = {\n",
    "        'boosting_type':'goss',\n",
    "        'learning_rate': 0.06733232950390658, \n",
    "        'n_estimators': 50000, \n",
    "        'early_stopping_round' : 100, \n",
    "        'subsample' : 0.6970532011679706,\n",
    "        'colsample_bytree': 0.6055755840633003,\n",
    "        'num_leaves': 6,\n",
    "        'class_weight': 'balanced',\n",
    "        'metric': 'none', \n",
    "        'is_unbalance': True, \n",
    "        'random_state': 8062023,\n",
    "        'feature_fraction_seed': 8062023,\n",
    "        'bagging_seed': 8062023,\n",
    "        'max_depth': 8,\n",
    "        'reg_alpha': 0.08866046540248787,  \n",
    "        'reg_lambda': 1.0245261859148395e-06,\n",
    "        'importance_type': 'gain'\n",
    "        }\n",
    "\n",
    "def balanced_log_loss(y_true, y_pred):\n",
    "\n",
    "    # Nc is the number of observations\n",
    "    N_1 = np.sum(y_true == 1, axis=0)\n",
    "    N_0 = np.sum(y_true == 0, axis=0)\n",
    "\n",
    "    # In order to avoid the extremes of the log function, each predicted probability ð‘ is replaced with max(min(ð‘,1âˆ’10âˆ’15),10âˆ’15)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # balanced logarithmic loss\n",
    "    loss_numerator = - (1/N_0) * np.sum((1 - y_true) * np.log(1 - y_pred)) - (1/N_1) * np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    return loss_numerator / 2\n",
    "\n",
    "def bll_metric(y_true, y_pred):\n",
    "    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n",
    "\n",
    "def lgbm_training():\n",
    "    models_ = list()\n",
    "    all_cv_score = list()\n",
    "    weights_ = list()\n",
    "    \n",
    "    all_eval_results_ = dict() # used to store evaluation results for each seed\n",
    "    X, y = train_df[features], train_df.Class\n",
    "#     X, y = generated_features_train, train_df.Class\n",
    "    \n",
    "    for seed in CFG.seeds:   \n",
    "        kf = MultilabelStratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=seed)\n",
    "        metric = balanced_log_loss\n",
    "        eval_results_ = {}     # used to store evaluation results for each fold\n",
    "        oof = np.zeros(len(X))\n",
    "        print(f\"Training with {blu}{X.shape[1]}{res} features, seed = {blu}{seed}{res}\")\n",
    "        \n",
    "        for fold, (fit_idx, val_idx) in enumerate(kf.split(X=train_df, y=greeks.iloc[:,1:3]), start = 1):\n",
    "            # Split the dataset according to the fold indexes.\n",
    "            X_train = X.iloc[fit_idx]\n",
    "            X_val = X.iloc[val_idx]\n",
    "            y_train = y.iloc[fit_idx]\n",
    "            y_val = y.iloc[val_idx]\n",
    "\n",
    "#             trn_w0, trn_w1 = calc_log_loss_weight(y_train)\n",
    "#             val_w0, val_w1 = calc_log_loss_weight(y_val)\n",
    "\n",
    "#             w_val = [y_train.map({0: trn_w0, 1: trn_w1}), y_val.map({0: val_w0, 1: val_w1})]\n",
    "\n",
    "            clf = lgb.LGBMClassifier(**params)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], \n",
    "                    eval_metric=bll_metric, early_stopping_rounds=300,\n",
    "                    verbose=-1)\n",
    "\n",
    "            eval_results_[fold]= clf.evals_result_\n",
    "            \n",
    "            val_preds = clf.predict_proba(X_val)[:,1]\n",
    "            oof[val_idx] = val_preds\n",
    "\n",
    "            val_score = balanced_log_loss(y_val, val_preds)\n",
    "            best_iter = clf.best_iteration_\n",
    "            \n",
    "            print(f'Fold: {blu}{fold:>3}{res}| {metric.__name__}: {blu}{val_score:.5f}{res}'\n",
    "                  f' | Best iteration: {blu}{best_iter:>4}{res}')\n",
    "\n",
    "            # Stores the model\n",
    "            models_.append(clf)\n",
    "            \n",
    "            # Stores evaluation result\n",
    "            all_eval_results_[seed] = eval_results_\n",
    "            \n",
    "            # Store prediction weight\n",
    "            weights_.append(1/clf.evals_result_['valid_1']['balanced_log_loss'][clf.best_iteration_])\n",
    "            \n",
    "#         oof[(oof > 0.8) & (oof < 0.9)] = 0.9\n",
    "#         oof[(oof < 0.2) & (oof > 0.1)] = 0.1\n",
    "        mean_cv_score = metric(y, oof)\n",
    "        \n",
    "        \n",
    "        all_cv_score.append(mean_cv_score)\n",
    "        print(f'{red}Mean{res} {metric.__name__}: {red}{mean_cv_score:.5f}')\n",
    "        print(f'{\"*\" * 50}\\n')\n",
    "    \n",
    "    print(f'{red}Mean{res} {metric.__name__} for all seeds: {red}{np.mean(all_cv_score):.5f}')\n",
    "    return models_, eval_results_, all_eval_results_, weights_\n",
    "\n",
    "models_, eval_results_, all_eval_results_, weights_ = lgbm_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean balanced_log_loss for all seeds: 0.18615"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T15:32:38.350522Z",
     "iopub.status.idle": "2023-06-08T15:32:38.351460Z",
     "shell.execute_reply": "2023-06-08T15:32:38.351239Z",
     "shell.execute_reply.started": "2023-06-08T15:32:38.351217Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## Model Evaluation\n",
    "# metric_score_folds = pd.DataFrame.from_dict(all_eval_results_)\n",
    "# fit_logloss = []\n",
    "# val_logloss = []\n",
    "\n",
    "# for seed in CFG.seeds:\n",
    "#     for fold in range(1,CFG.n_folds+1):\n",
    "#         fit_logloss.append(metric_score_folds[seed][fold]['training']['balanced_log_loss'])\n",
    "#         val_logloss.append(metric_score_folds[seed][fold]['valid_1']['balanced_log_loss'])\n",
    "\n",
    "# fig, axes = plt.subplots(math.ceil(CFG.n_folds*len(CFG.seeds)/CFG.n_folds), CFG.n_folds, figsize=(20, 20), dpi=150)\n",
    "# ax = axes.flatten()\n",
    "# for i, (f, v, m) in enumerate(zip(fit_logloss, val_logloss, models_), start = 1): \n",
    "#     sns.lineplot(f, color='#B90000', ax=ax[i-1], label='fit')\n",
    "#     sns.lineplot(v, color='#048BA8', ax=ax[i-1], label='val')\n",
    "#     ax[i-1].legend()\n",
    "#     ax[i-1].spines['top'].set_visible(False);\n",
    "#     ax[i-1].spines['right'].set_visible(False)\n",
    "#     ax[i-1].set_title(f'Seed {CFG.seeds[(i-1)//CFG.n_folds]} Fold {CFG.n_folds if i%CFG.n_folds==0 else i%CFG.n_folds}', fontdict={'fontweight': 'bold'})\n",
    "\n",
    "#     color =  ['#048BA8', palette[-3]]\n",
    "#     best_iter = m.best_iteration_\n",
    "#     span_range = [[0, best_iter], [best_iter + 10, best_iter + CFG.num_boost_round]]\n",
    "\n",
    "#     for idx, sub_title in enumerate([f'Best\\nIteration: {best_iter}', f'Early\\n Stopping: 2000']):\n",
    "#         ax[i-1].annotate(sub_title,\n",
    "#                     xy=(sum(span_range[idx])/2 , 0.5),\n",
    "#                     xytext=(0,0), textcoords='offset points',\n",
    "#                     va=\"center\", ha=\"center\",\n",
    "#                     color=\"w\", fontsize=16, fontweight='bold',\n",
    "#                     bbox=dict(boxstyle='round4', pad=0.4, color=color[idx], alpha=0.6))\n",
    "#         ax[i-1].axvspan(span_range[idx][0]-0.4,span_range[idx][1]+0.4,  color=color[idx], alpha=0.07)\n",
    "\n",
    "#     ax[i-1].set_xlim(0, best_iter + 20 + 2000)\n",
    "#     ax[i-1].legend(bbox_to_anchor=(0.95, 1), loc='upper right', title='logloss')\n",
    "\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-06-08T15:32:38.352845Z",
     "iopub.status.idle": "2023-06-08T15:32:38.353222Z",
     "shell.execute_reply": "2023-06-08T15:32:38.353059Z",
     "shell.execute_reply.started": "2023-06-08T15:32:38.353042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00eed32682bb</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>010ebe33f668</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02fa521e1838</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>040e15f562a2</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>046e85c7cc7f</td>\n",
       "      <td>0.590987</td>\n",
       "      <td>0.409013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Id   class_0   class_1\n",
       "0  00eed32682bb  0.590987  0.409013\n",
       "1  010ebe33f668  0.590987  0.409013\n",
       "2  02fa521e1838  0.590987  0.409013\n",
       "3  040e15f562a2  0.590987  0.409013\n",
       "4  046e85c7cc7f  0.590987  0.409013"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_preds = np.zeros((test_df.shape[0],2))\n",
    "\n",
    "# for i in range(len(final_test_predictions)):\n",
    "#     test_preds[:, 0] += test_weights[i] * final_test_predictions[i][:, 0]\n",
    "#     test_preds[:, 1] += test_weights[i] * final_test_predictions[i][:, 1]\n",
    "\n",
    "# test_preds /= sum(test_weights)\n",
    "\n",
    "def predict(X):\n",
    "    y = np.zeros(len(X))\n",
    "    for i, model in enumerate(models_):\n",
    "#         y += weights_[i] * model.predict_proba(X)[:,1]\n",
    "        y += model.predict_proba(X)[:,1]\n",
    "#     return y / sum(weights_)\n",
    "    return y / len(models_)\n",
    "\n",
    "predictions = predict(test_df[features])\n",
    "# predictions = predict(generated_features_test)\n",
    "\n",
    "test_df['class_1'] = predictions\n",
    "test_df['class_0'] = 1 - predictions\n",
    "\n",
    "sample_submission[['class_0', 'class_1']] = test_df[['class_0', 'class_1']]\n",
    "sample_submission.to_csv(r\"submission.csv\", index=False)\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a lot of resulting features. I have already identified a few important once. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
