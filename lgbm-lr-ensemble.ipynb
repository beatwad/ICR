{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-04T10:02:19.444571Z","iopub.status.busy":"2023-08-04T10:02:19.443355Z","iopub.status.idle":"2023-08-04T10:02:56.480117Z","shell.execute_reply":"2023-08-04T10:02:56.478442Z","shell.execute_reply.started":"2023-08-04T10:02:19.444516Z"},"trusted":true},"outputs":[],"source":["# !pip install tabpfn --no-index --find-links=file:///kaggle/input/tab-pfn-dataset\n","# !mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n","# !cp /kaggle/input/tab-pfn-dataset/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/\n","\n","# !pip install adjdatatools --no-index --find-links=file:///kaggle/input/adjdatatools\n","# !pip -q install featurewiz --no-index --find-links=file:///kaggle/input/featurewiz"]},{"cell_type":"code","execution_count":2,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-08-04T10:02:56.484487Z","iopub.status.busy":"2023-08-04T10:02:56.483928Z","iopub.status.idle":"2023-08-04T10:02:56.506724Z","shell.execute_reply":"2023-08-04T10:02:56.505146Z","shell.execute_reply.started":"2023-08-04T10:02:56.484437Z"},"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append('/kaggle/input/iter-strat/iter_strat')\n","\n","import math\n","import copy \n","\n","import numpy as np\n","import pandas as pd\n","\n","import lightgbm as lgb\n","import catboost as cat\n","from catboost import Pool\n","import xgboost as xgb\n","from tabpfn import TabPFNClassifier\n","from sklearn.linear_model import LogisticRegression, LinearRegression\n","import category_encoders as encoders\n","\n","import itertools\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.over_sampling import RandomOverSampler\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n","from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, BaseShuffleSplit, _validate_shuffle_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import RFE\n","from sklearn.metrics import log_loss\n","from sklearn.model_selection import GridSearchCV, KFold\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from adjdatatools.preprocessing import AdjustedScaler\n","from sklearn.feature_selection import SelectKBest, f_classif\n","\n","from sklearn.model_selection import GridSearchCV, KFold\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","from sklearn.utils import check_random_state\n","from sklearn.utils.validation import _num_samples, check_array\n","from sklearn.utils.multiclass import type_of_target\n","\n","from scipy import stats\n","\n","import eli5\n","from IPython.display import display\n","from eli5.permutation_importance import get_score_importances\n","from eli5.sklearn import PermutationImportance\n","\n","import matplotlib.pyplot as plt\n","from tqdm.auto import tqdm\n","import seaborn as sns\n","\n","import optuna\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_columns', 100)\n","\n","\n","from colorama import Style, Fore\n","\n","palette = ['#302c36', '#037d97', '#E4591E', '#C09741',\n","           '#EC5B6D', '#90A6B1', '#6ca957', '#D8E3E2']\n","\n","blk = Style.BRIGHT + Fore.BLACK\n","red = Style.BRIGHT + Fore.RED\n","blu = Style.BRIGHT + Fore.BLUE\n","res = Style.RESET_ALL\n","\n","\n","class CFG:\n","    # main\n","    kaggle = False\n","    test = False\n","    \n","    # features\n","    fe_drop = True\n","    \n","    fix_errs = False\n","    err_objs = [292, 102, 509, 367, 313, 380, 556]\n","\n","    del_outliers = False\n","    del_outliers_adj = True\n","    \n","    feature_sel = False\n","    n_feature_sel_repeats = 5\n","    n_feature_sel_folds = 5\n","    \n","    undersample = False\n","    oversample = False\n","    \n","    nan_impute = True\n","    encode_cat = False\n","    standard_scale = False\n","    log = False\n","    \n","    # optimization\n","    n_estimators = 3000\n","    early_stopping_rounds = 100\n","    \n","    lgbm_optimize = False\n","    lgbm_optimize2 = False\n","    xgb_optimize = False\n","    cb_optimize = False\n","    \n","    n_trials = 500\n","    n_optimize_folds = 5\n","    n_optimize_repeats = 3\n","    \n","    # train\n","    k_fold = False\n","    strat_k_fold = True\n","    add_err_objs = False\n","    select_best_fold = False\n","    \n","    lgbm_train = True\n","    xgb_train = False\n","    cb_train = False\n","    tabpfn_train = False\n","    logreg_train = False\n","\n","    # inference\n","    n_stacking_folds = 5\n","    n_stacking_folds_min = 1\n","    n_stacking_folds_max = 5\n","\n","    n_stacking_models_lgbm = 5\n","    n_stacking_models_xgb = 10\n","    n_stacking_models_cb = 5\n","    n_stacking_models_tabpfn = 20\n","\n","    adjust_class_threshold = False\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Load Data"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-08-04T10:02:56.508266Z","iopub.status.busy":"2023-08-04T10:02:56.507888Z","iopub.status.idle":"2023-08-04T10:02:56.557380Z","shell.execute_reply":"2023-08-04T10:02:56.555872Z","shell.execute_reply.started":"2023-08-04T10:02:56.508236Z"},"trusted":true},"outputs":[],"source":["if CFG.kaggle:\n","    COMP_PATH = \"/kaggle/input/icr-identify-age-related-conditions\"\n","else:\n","    COMP_PATH = \"icr-identify-age-related-conditions\"\n","\n","train_df = pd.read_csv(f'{COMP_PATH}/train.csv')\n","test_df = pd.read_csv(f'{COMP_PATH}/test.csv')\n","greeks = pd.read_csv(f\"{COMP_PATH}/greeks.csv\")\n","sample_submission = pd.read_csv(f\"{COMP_PATH}/sample_submission.csv\")\n","\n","train_df['EJ'] = train_df['EJ'].replace({'A': 0, 'B': 1})\n","test_df['EJ'] = test_df['EJ'].replace({'A': 0, 'B': 1})\n","test_df['EJ'] = test_df['EJ'].fillna(0)\n","\n","train_df.columns = train_df.columns.str.replace(' ', '')\n","test_df.columns = test_df.columns.str.replace(' ', '')"]},{"cell_type":"markdown","metadata":{},"source":["# NaN impute"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-04T10:02:56.562285Z","iopub.status.busy":"2023-08-04T10:02:56.561802Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 10 folds for each of 18 candidates, totalling 180 fits\n","Fitting 10 folds for each of 18 candidates, totalling 180 fits\n"]}],"source":["if CFG.nan_impute:\n","    #EL datasets\n","    train_el_df = train_df[~train_df.EL.isna()]\n","    X_train_el_df=train_el_df.drop(['BQ', 'EL', 'Class', 'Id'], axis=1)\n","    y_train_el_df=train_el_df.EL\n","\n","    val_el_df = train_df[train_df.EL.isna()]\n","    X_val_el_df = val_el_df.drop(['BQ', 'EL', 'Class', 'Id'], axis=1)\n","\n","#     test_df['EL'] = [0, 0, np.nan, 0, np.nan]\n","#     test_df['CU'] = [np.nan, 0, np.nan, np.nan, 0]\n","    \n","    test_el_df = test_df[test_df.EL.isna()]\n","    X_test_el_df = test_el_df.drop(['BQ', 'EL', 'Id'], axis=1)\n","\n","    #making grid for hyperparamters optimization for feature selection\n","    el_grid_fs = GridSearchCV(xgb.XGBRegressor(), param_grid={'n_estimators':[50,80,100], 'eta': [0.001, 0.005, 0.01, 0.03, 0.1, 1]},\n","                            n_jobs=-1, cv=10, verbose=1,scoring='neg_mean_squared_error')\n","    el_grid_fs.fit(X_train_el_df, y_train_el_df)\n","\n","    #making model for features selection\n","    el_model_fs = xgb.XGBRegressor(n_estimators=el_grid_fs.best_params_['n_estimators'], eta=el_grid_fs.best_params_['eta'])\n","    el_model_fs.fit(X_train_el_df, y_train_el_df)\n","\n","    #chosing 10 most important features\n","    feature_importances = el_model_fs.feature_importances_\n","    sorted_indices = np.argsort(feature_importances)[::-1]\n","    features_el = sorted_indices[:10]\n","\n","    X_train_el_df = X_train_el_df.iloc[:,features_el]\n","    X_val_el_df = X_val_el_df.iloc[:,features_el]\n","    X_test_el_df = X_test_el_df.iloc[:,features_el]\n","\n","    #making grid for hyperparamters optimization for prediction\n","    el_grid = GridSearchCV(xgb.XGBRegressor(), param_grid={'n_estimators':[50,80,100], 'eta': [0.001, 0.005, 0.01, 0.03, 0.1, 1]},\n","                            n_jobs=-1, cv=10, verbose=1, scoring='neg_mean_squared_error')\n","    el_grid.fit(X_train_el_df, y_train_el_df)\n","\n","    #making model for prediction\n","    el_model = xgb.XGBRegressor(n_estimators=el_grid.best_params_['n_estimators'], eta=el_grid.best_params_['eta'])\n","    el_model.fit(X_train_el_df, y_train_el_df)\n","\n","    el_pred = el_model.predict(X_val_el_df)\n","    train_df.loc[train_df.EL.isna(), 'EL'] = el_pred\n","\n","    if X_test_el_df.shape[0] > 0:\n","        try:\n","            el_pred_test = el_model.predict(X_test_el_df)\n","            test_df.loc[test_df.EL.isna(), 'EL'] = el_pred_test\n","        except:\n","            el_pred_test = el_model.predict(X_test_el_df.fillna(X_test_el_df.mean()))\n","            test_df.loc[test_df.EL.isna(), 'EL'] = el_pred_test"]},{"cell_type":"markdown","metadata":{},"source":["# Drop not necessary features"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["(58, 38, 55)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["if CFG.fe_drop:\n","    features = [fe for fe in train_df.columns if fe not in ['CF', 'CB', 'DV', 'BR', 'DF', 'GB', 'AH',\n","                                                            'CW', 'CL', 'BP', 'BD', 'FC', 'GE', 'GF',\n","                                                            'AR', 'GI', 'Id', 'Class', 'AX', 'DA']]\n","else:\n","    features = [fe for fe in train_df.columns if fe not in ['Id', 'Class', 'EJ']]\n","\n","num_cols = [nc for nc in train_df.select_dtypes(include=['float64']).columns if nc != 'Class']\n","    \n","# clip values to avoid different values in the test set from train !!!\n","# test_df[features] = test_df[features].clip(train_df[features].min(axis=0).values, train_df[features].max(axis=0).values, axis=1)\n","    \n","len(train_df.columns), len(features), len(num_cols)"]},{"cell_type":"markdown","metadata":{},"source":["# Delete outliers"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["features_with_outliers = [fe for fe in train_df.columns if fe not in ['BN', 'BQ', 'CW', 'EL', 'GH', \n","                                                                      'GI', 'GL', 'Id', 'Class', 'EJ']]\n","\n","if CFG.del_outliers:\n","    for f in features_with_outliers:\n","        train_df[f] = train_df[f].clip(upper=train_df[f].quantile(0.99))\n","        test_df[f] = test_df[f].clip(upper=test_df[f].quantile(0.99))\n","\n","if CFG.del_outliers_adj:\n","    adj_scaler = AdjustedScaler(with_centering=True)\n","    # adj_features = [f for f in features if f != 'EJ']\n","    adj_features = ['EL']\n","    \n","    adj_scaler.fit(train_df[adj_features])\n","    train_df['EL_adj'] = adj_scaler.transform(train_df[adj_features])\n","    test_df['EL_adj'] = adj_scaler.transform(test_df[adj_features])"]},{"cell_type":"markdown","metadata":{},"source":["# Load LGBM parameters"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["import glob\n","\n","def load_lgbm_parameters(filename):\n","    param_list = glob.glob(filename)\n","\n","    models = list()\n","    best_lgbm_params = list()\n","\n","    lgbm_params = pd.DataFrame()\n","\n","    for f in param_list:\n","        tmp = pd.read_csv(f, index_col='Unnamed: 0')\n","        if lgbm_params.shape[0] == 0:\n","            lgbm_params = tmp\n","        else:\n","            lgbm_params = pd.concat([lgbm_params, tmp])\n","            \n","    lgbm_params = lgbm_params.sort_values('value').head(CFG.n_stacking_models_lgbm)\n","    param_cols = [c for c in lgbm_params.columns if c.startswith('params_')]\n","    lgbm_params = lgbm_params[param_cols]\n","\n","    for idx, row in lgbm_params.iterrows():\n","        row_dict = {k[7:]: v for k, v in row.items()}\n","        row_dict['objective'] = 'binary'\n","        row_dict['metric'] = 'none'\n","    #     row_dict['subsample_for_bin'] = 300000\n","        row_dict['force_col_wise'] = False\n","        row_dict['verbose'] = -1\n","        # row_dict['max_bin'] = 255\n","        \n","        if CFG.n_stacking_folds > 0:\n","            row_dict['n_estimators'] = CFG.n_estimators\n","            row_dict['early_stopping_round'] = CFG.early_stopping_rounds\n","        else:\n","            row_dict['n_estimators'] = int(row_dict['n_estimators'])\n","        row_dict['num_leaves'] = int(row_dict['num_leaves'])\n","        row_dict['max_depth'] = int(row_dict['max_depth'])\n","        row_dict['min_child_samples'] = int(row_dict['min_child_samples'])\n","        row_dict['subsample_freq'] = int(row_dict['subsample_freq'])\n","        row_dict['learning_rate'] = float(row_dict['learning_rate'])\n","        row_dict['max_bin'] = int(row_dict['max_bin'])\n","        \n","        if not CFG.oversample and not CFG.undersample:\n","            row_dict['is_unbalance'] = True\n","            row_dict['class_weight'] = 'balanced'\n","        else:\n","            row_dict['scale_pos_weight'] = class_imbalance\n","        \n","        if row_dict['boosting_type'] == 'goss':\n","            row_dict['subsample'] = None\n","            \n","        best_lgbm_params.append(row_dict)\n","    return best_lgbm_params\n","\n","best_lgbm_params = load_lgbm_parameters('optuna_lgbm.csv')\n","\n","if CFG.test:\n","    best_lgbm_params = [{\n","            'boosting_type': 'goss',\n","            'n_estimators': 50000,\n","            'early_stopping_round': 300,\n","            'max_depth': 8,\n","            'learning_rate': 0.06733232950390658,\n","            'subsample': 0.6970532011679706,\n","            'colsample_bytree': 0.6055755840633003,\n","            'is_unbalance': True, \n","            'class_weight': 'balanced',\n","            'metric':'none',\n","            'verbose': -1,\n","            'random_state': 42,\n","        }\n","    ]\n","\n","                         "]},{"cell_type":"markdown","metadata":{},"source":["# Train models"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34ce4766ce1d48b6a590874e4b67fbee","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training with \u001b[1m\u001b[34m38\u001b[0m features\n","Fold: \u001b[1m\u001b[34m  0\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.11412\u001b[0m | Best iteration: \u001b[1m\u001b[34m  80\u001b[0m\n","Fold: \u001b[1m\u001b[34m  1\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.15002\u001b[0m | Best iteration: \u001b[1m\u001b[34m 116\u001b[0m\n","Fold: \u001b[1m\u001b[34m  2\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.24423\u001b[0m | Best iteration: \u001b[1m\u001b[34m  75\u001b[0m\n","Fold: \u001b[1m\u001b[34m  3\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.27084\u001b[0m | Best iteration: \u001b[1m\u001b[34m  74\u001b[0m\n","Fold: \u001b[1m\u001b[34m  4\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.17767\u001b[0m | Best iteration: \u001b[1m\u001b[34m 169\u001b[0m\n","Training with \u001b[1m\u001b[34m38\u001b[0m features\n","Fold: \u001b[1m\u001b[34m  0\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.16482\u001b[0m | Best iteration: \u001b[1m\u001b[34m 112\u001b[0m\n","Fold: \u001b[1m\u001b[34m  1\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.16766\u001b[0m | Best iteration: \u001b[1m\u001b[34m  68\u001b[0m\n","Fold: \u001b[1m\u001b[34m  2\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.29180\u001b[0m | Best iteration: \u001b[1m\u001b[34m  69\u001b[0m\n","Fold: \u001b[1m\u001b[34m  3\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.25362\u001b[0m | Best iteration: \u001b[1m\u001b[34m  55\u001b[0m\n","Fold: \u001b[1m\u001b[34m  4\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.15666\u001b[0m | Best iteration: \u001b[1m\u001b[34m 174\u001b[0m\n","Training with \u001b[1m\u001b[34m38\u001b[0m features\n","Fold: \u001b[1m\u001b[34m  0\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.16715\u001b[0m | Best iteration: \u001b[1m\u001b[34m 151\u001b[0m\n","Fold: \u001b[1m\u001b[34m  1\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.15564\u001b[0m | Best iteration: \u001b[1m\u001b[34m 168\u001b[0m\n","Fold: \u001b[1m\u001b[34m  2\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.28431\u001b[0m | Best iteration: \u001b[1m\u001b[34m  85\u001b[0m\n","Fold: \u001b[1m\u001b[34m  3\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.26515\u001b[0m | Best iteration: \u001b[1m\u001b[34m  95\u001b[0m\n","Fold: \u001b[1m\u001b[34m  4\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.14435\u001b[0m | Best iteration: \u001b[1m\u001b[34m 292\u001b[0m\n","Training with \u001b[1m\u001b[34m38\u001b[0m features\n","Fold: \u001b[1m\u001b[34m  0\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.15044\u001b[0m | Best iteration: \u001b[1m\u001b[34m 278\u001b[0m\n","Fold: \u001b[1m\u001b[34m  1\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.17478\u001b[0m | Best iteration: \u001b[1m\u001b[34m 325\u001b[0m\n","Fold: \u001b[1m\u001b[34m  2\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.25324\u001b[0m | Best iteration: \u001b[1m\u001b[34m 195\u001b[0m\n","Fold: \u001b[1m\u001b[34m  3\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.28593\u001b[0m | Best iteration: \u001b[1m\u001b[34m 191\u001b[0m\n","Fold: \u001b[1m\u001b[34m  4\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.14098\u001b[0m | Best iteration: \u001b[1m\u001b[34m 551\u001b[0m\n","Training with \u001b[1m\u001b[34m38\u001b[0m features\n","Fold: \u001b[1m\u001b[34m  0\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.10669\u001b[0m | Best iteration: \u001b[1m\u001b[34m 239\u001b[0m\n","Fold: \u001b[1m\u001b[34m  1\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.13918\u001b[0m | Best iteration: \u001b[1m\u001b[34m 257\u001b[0m\n","Fold: \u001b[1m\u001b[34m  2\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.24363\u001b[0m | Best iteration: \u001b[1m\u001b[34m 145\u001b[0m\n","Fold: \u001b[1m\u001b[34m  3\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.26432\u001b[0m | Best iteration: \u001b[1m\u001b[34m 144\u001b[0m\n","Fold: \u001b[1m\u001b[34m  4\u001b[0m| bll_metric: \u001b[1m\u001b[34m0.11590\u001b[0m | Best iteration: \u001b[1m\u001b[34m 468\u001b[0m\n"]}],"source":["def balanced_log_loss(y_true, y_pred):\n","    # Nc is the number of observations\n","    N_1 = np.sum(y_true == 1, axis=0)\n","    N_0 = np.sum(y_true == 0, axis=0)\n","\n","    N_inv_0 = 1/N_0 if N_0 > 0 else 0\n","    N_inv_1 = 1/N_1 if N_1 > 0 else 0\n","\n","    # In order to avoid the extremes of the log function, each predicted probability ð‘ is replaced with max(min(ð‘,1âˆ’10âˆ’15),10âˆ’15)\n","    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n","\n","    # balanced logarithmic loss\n","    loss_numerator = - N_inv_0 * np.sum((1 - y_true) * np.log(1 - y_pred)) - N_inv_1 * np.sum(y_true * np.log(y_pred))\n","\n","    return loss_numerator / 2\n","\n","def bll_metric(y_true, y_pred):\n","    return 'balanced_log_loss', balanced_log_loss(y_true, y_pred), False\n","\n","def pp_prob(p):\n","    c0 = p[:,0].sum()\n","    c1 = p[:,1:].sum()\n","    new_p = p * np.array([[1/(c0 if i==0 else c1) for i in range(p.shape[1])]])\n","    new_p = new_p / np.sum(new_p,axis=1,keepdims=1)\n","    return np.sum(new_p[:,1:],1,keepdims=False)\n","\n","def model_train(how, best_params, X, y, test):\n","    oof_level2 = np.zeros([y.shape[0], len(best_params) + 1])\n","    oof_level2[:, len(best_params)] = y\n","    oof_level2_test = np.zeros([test_df.shape[0], len(best_params)])\n","    oof_val = np.zeros([CFG.n_stacking_folds, len(best_params)])\n","    \n","    for i, params in tqdm(enumerate(best_params), total=len(best_params)):\n","        model_dict = dict()\n","    \n","        if CFG.n_stacking_folds > 0:\n","            if CFG.k_fold:\n","                kf = KFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=3082023)\n","                y_fold = y\n","            elif CFG.strat_k_fold:\n","                kf = StratifiedKFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=3082023)\n","                y_fold = y\n","            else:\n","                kf = MultilabelStratifiedKFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=3082023)\n","                y_fold = greeks.iloc[:,1:4]\n","            \n","            print(f\"Training with {blu}{len(features)}{res} features\")\n","\n","            best_val = np.inf\n","            \n","            for fold, (fit_idx, val_idx) in enumerate(kf.split(X=X, y=y_fold)):\n","                \n","                # Split the dataset according to the fold indexes.\n","                X_train = X.iloc[fit_idx]\n","                X_val = X.iloc[val_idx]\n","                y_train = y.iloc[fit_idx]\n","                y_val = y.iloc[val_idx]\n","\n","                # Make random under- or oversampling to balance classes\n","                if CFG.undersample or CFG.oversample:\n","                    if CFG.undersample:\n","                        positive_count_train = y_train.value_counts()[1]\n","                        sampler = RandomUnderSampler(sampling_strategy={0: positive_count_train * class_imbalance, \n","                                                                        1: positive_count_train}, \n","                                                    random_state=3082023, \n","                                                    replacement=True)\n","                    elif CFG.oversample:\n","                        negative_count_train = y_train.value_counts()[0]\n","                        sampler = RandomOverSampler(sampling_strategy={0: negative_count_train, \n","                                                                    1: negative_count_train // class_imbalance}, \n","                                                    random_state=3082023)\n","\n","                    X_train, y_train = sampler.fit_resample(X_train, y_train)\n","                \n","                if how == 'lgbm':\n","                    model = lgb.LGBMClassifier(**params)\n","                    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric=bll_metric, verbose=0)\n","                    best_iter = model.best_iteration_\n","                elif how == 'xgboost':\n","                    model = xgb.XGBClassifier(**params)\n","                    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=0)\n","                    best_iter = model.get_booster().best_iteration\n","                elif how == 'catboost':\n","                    train_pool = Pool(X_train, y_train, cat_features=['EJ'])\n","                    val_pool = Pool(X_val, y_val, cat_features=['EJ'])   \n","                    model = cat.CatBoostClassifier(**params)\n","                    model.fit(train_pool, eval_set=val_pool, verbose=0)\n","                    best_iter = model.best_iteration_\n","                elif how == 'tabpfn':\n","                    model = TabPFNClassifier(N_ensemble_configurations=64, device='cuda:0')\n","                    model.fit(X_train, y_train, overwrite_warning=True)\n","                    best_iter = 0\n","                elif how == 'logreg':\n","                    model = LogisticRegression(random_state=2306020231+i, C=0.1, n_jobs=-1, max_iter=2000, class_weight='balanced')\n","                    model.fit(X_train, y_train)\n","                    best_iter = 0\n","                else:\n","                    return None, None, None\n","                    \n","                try:\n","                    if how == 'tabpfn':\n","                        val_preds = pp_prob(model.predict_proba(X_val))\n","                        val_score = balanced_log_loss(y_val, val_preds)\n","                        pp = pp_prob(model.predict_proba(test))\n","                    else:\n","                        val_preds = model.predict_proba(X_val)[:,1]\n","                        val_score = balanced_log_loss(y_val, val_preds)\n","                        pp = model.predict_proba(test)[:,1]\n","                except:\n","                    val_score = 100\n","                    pp = np.zeros(test.shape[0])\n","\n","                model_dict[val_score] = pp\n","                \n","                oof_level2[val_idx, i] = val_preds\n","                oof_val[fold, i] = val_score\n","                \n","                print(f'Fold: {blu}{fold:>3}{res}| bll_metric: {blu}{val_score:.5f}{res}'\n","                      f' | Best iteration: {blu}{best_iter:>4}{res}')  \n","            \n","            model_dict = sorted(model_dict.items(), key=lambda x: x[0])\n","            \n","            n_stacking_folds = CFG.n_stacking_folds\n","            \n","            for j, _pp in enumerate(model_dict):\n","                # if j >= CFG.n_stacking_folds_min or _pp[0] >= 0.1:\n","                if _pp[0] >= 0.1:\n","                    oof_level2_test[:, i] += _pp[1]\n","                else:\n","                    n_stacking_folds -= 1\n","            oof_level2_test[:, i] = oof_level2_test[:, i] / max(1, n_stacking_folds)\n","        else:\n","            if how == 'lgbm':\n","                model = lgb.LGBMClassifier(**params)\n","                model.fit(X, y, verbose=0)\n","            elif how == 'xgboost':\n","                model = xgb.XGBClassifier(**params)\n","                model.fit(X, y, verbose=0)\n","            elif how == 'catboost':\n","                train_pool = Pool(X, y, cat_features=['EJ'])\n","                model = cat.CatBoostClassifier(**params)\n","                model.fit(train_pool, verbose=0)\n","            elif how == 'tabpfn':\n","                model = TabPFNClassifier(N_ensemble_configurations=64, device='cuda:0')\n","                model.fit(X, y, overwrite_warning=True)\n","            elif how == 'logreg':\n","                model = LogisticRegression(random_state=2306020231+i, C=0.1, n_jobs=-1, max_iter=2000, class_weight='balanced')\n","                model.fit(X, y)\n","            else:\n","                return None, None, None\n","    \n","    return oof_level2, oof_level2_test, oof_val\n","\n","oof_train_list = list()\n","oof_test_list = list()\n","oof_val_list = list()\n","\n","if CFG.lgbm_train:\n","    oof_level2_lgbm, oof_level2_test_lgbm, oof_val_lgbm = model_train('lgbm', best_lgbm_params, train_df[features], train_df['Class'], test_df[features])\n","    oof_train_list.append(oof_level2_lgbm[:,:-1])\n","    oof_test_list.append(oof_level2_test_lgbm)\n","    oof_val_list.append(oof_val_lgbm)\n","    y = oof_level2_lgbm[:,-1]\n","\n","if CFG.xgb_train:\n","    oof_level2_xgb, oof_level2_test_xgb, oof_val_xgb = model_train('xgboost', best_xgb_params, train_df[features], train_df['Class'], test_df[features])\n","    oof_train_list.append(oof_level2_xgb[:,:-1])\n","    oof_test_list.append(oof_level2_test_xgb)\n","    oof_val_list.append(oof_val_xgb)\n","    y = oof_level2_xgb[:,-1]\n","\n","if CFG.cb_train:\n","    oof_level2_cb, oof_level2_test_cb, oof_val_cb = model_train('catboost', best_cb_params, train_df[features], train_df['Class'], test_df[features])\n","    oof_train_list.append(oof_level2_cb[:,:-1])\n","    oof_test_list.append(oof_level2_test_cb)\n","    oof_val_list.append(oof_val_cb)\n","    y = oof_level2_cb[:,-1]\n","\n","if CFG.tabpfn_train:\n","    oof_level2_tabpfn, oof_level2_test_tabpfn, oof_val_tabpfn = model_train('tabpfn', [i for i in range(CFG.n_stacking_models_tabpfn)], \n","                                                                             train_df[features], train_df['Class'], test_df[features])\n","    oof_train_list.append(oof_level2_tabpfn[:,:-1])\n","    oof_test_list.append(oof_level2_test_tabpfn)\n","    oof_val_list.append(oof_val_tabpfn)\n","    y = oof_level2_tabpfn[:,-1]\n","\n","# if CFG.logreg_train:\n","#     oof_level2_logreg, oof_level2_test_logreg, oof_val_logreg = model_train('logreg', [i for i in range (10)])\n","#     oof_train_list.append(oof_level2_logreg[:,:-1])\n","#     oof_test_list.append(oof_level2_test_logreg)\n","#     oof_val_list.append(oof_val_logreg)\n","#     y = oof_level2_logreg[:,-1]"]},{"cell_type":"markdown","metadata":{},"source":["# Blending and Stacking with Logistic Regression"]},{"cell_type":"code","execution_count":9,"metadata":{"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.1839780951638027\n","0.1793033734474975\n"]}],"source":["oof_level2_LGBM = np.concatenate(oof_train_list, axis=1)\n","oof_level2_LGBM_test = np.concatenate(oof_test_list, axis=1)\n","# oof_level2_val = np.concatenate(oof_val_list, axis=1).reshape(-1, )\n","\n","X = oof_level2_LGBM\n","\n","# mean bll\n","print(balanced_log_loss(y, np.mean(X, axis=1)))\n","\n","# lr bll\n","lr = LogisticRegression(class_weight='balanced')\n","lr.fit(X, y)\n","\n","pred = lr.predict_proba(X)[:,1]\n","weights = lr.coef_[0]\n","print(balanced_log_loss(y, (weights * X).sum(axis=1) / sum(weights)))"]},{"cell_type":"markdown","metadata":{},"source":["# Use Optuna to calculate model weights"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0.1728000019463083"]},"metadata":{},"output_type":"display_data"}],"source":["from functools import partial\n","\n","class OptunaWeights:\n","    def __init__(self, random_state, n_trials=2000):\n","        self.study = None\n","        self.weights = None\n","        self.random_state = random_state\n","        self.n_trials = n_trials\n","\n","    def _objective(self, trial, y_true, y_preds):\n","        # Define the weights for the predictions from each model\n","        weights = [trial.suggest_float(f\"weight{n}\", 1e-13, 1) for n in range(len(y_preds))]\n","\n","        # Calculate the weighted prediction\n","        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n","\n","        # Calculate the score for the weighted prediction\n","        # score = log_loss(y_true, weighted_pred)\n","        score = balanced_log_loss(y_true, weighted_pred)\n","        return score\n","\n","    def fit(self, y_true, y_preds):\n","        optuna.logging.set_verbosity(optuna.logging.ERROR)\n","        sampler = optuna.samplers.CmaEsSampler(seed=self.random_state)\n","        pruner = optuna.pruners.HyperbandPruner()\n","        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='minimize')\n","        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n","        self.study.optimize(objective_partial, n_trials=self.n_trials)\n","        self.weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n","\n","    def predict(self, y_preds):\n","        assert self.weights is not None, 'OptunaWeights error, must be fitted before predict'\n","        weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=self.weights)\n","        return weighted_pred\n","\n","    def fit_predict(self, y_true, y_preds):\n","        self.fit(y_true, y_preds)\n","        return self.predict(y_preds)\n","    \n","    def weights(self):\n","        return self.weights\n","\n","# Use Optuna to find the best ensemble weights\n","optweights = OptunaWeights(random_state=19072023)\n","y_val_pred = optweights.fit_predict(y, [oof_level2_LGBM[:,i] for i in range(oof_level2_LGBM.shape[1])])\n","optuna_weights_LGBM = np.array(optweights.weights)\n","display(balanced_log_loss(y, y_val_pred))\n","\n","oof_level2_LGBM = (optuna_weights_LGBM * oof_level2_LGBM).sum(axis=1) / sum(optuna_weights_LGBM)\n","oof_level2_LGBM_test = (optuna_weights_LGBM * oof_level2_LGBM_test).sum(axis=1) / sum(optuna_weights_LGBM)\n","\n","# oof_level2_LGBM = oof_level2_LGBM.mean(axis=1)\n","# oof_level2_LGBM_test = oof_level2_LGBM_test.mean(axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["# =================================================================\n","# Logistic Regression\n","\n","# Load data"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[],"source":["train = pd.read_csv(f'{COMP_PATH}/train.csv')\n","test = pd.read_csv(f'{COMP_PATH}/test.csv')\n","greeks = pd.read_csv(f'{COMP_PATH}/greeks.csv')\n","\n","train.columns = train.columns.str.replace(' ', '')\n","test.columns = test.columns.str.replace(' ', '')"]},{"cell_type":"markdown","metadata":{},"source":["# Greeks will be used in the stratified k-fold strategy"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["greeks['k'] = greeks['Alpha'] + greeks['Beta'] + greeks['Gamma'] + greeks['Delta']\n","train = pd.merge( greeks[['k', 'Id']],train,on='Id')\n","\n","names = ['AB', 'AF', 'AH', 'AM', 'AR', 'AX', 'AY', 'AZ', 'BC', 'BD', 'BN',\n","         'BP', 'BQ', 'BR', 'BZ', 'CB', 'CC', 'CD', 'CF', 'CH', 'CL', 'CR', 'CS',\n","         'CU', 'CW', 'DA', 'DE', 'DF', 'DH', 'DI', 'DL', 'DN', 'DU', 'DV', 'DY',\n","         'EB', 'EE', 'EG', 'EH', 'EJ', 'EL', 'EP', 'EU', 'FC', 'FD', 'FE', 'FI',\n","         'FL', 'FR', 'FS', 'GB', 'GE', 'GF', 'GH', 'GI', 'GL']\n","target_name = 'Class'"]},{"cell_type":"markdown","metadata":{},"source":["# Data Cleaning"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["train['EJ'] = pd.Series(np.where(train.EJ.values == 'A', 1, 0), train.index)\n","test['EJ'] = pd.Series(np.where(test.EJ.values == 'A', 1, 0), test.index)\n","\n","# fill nan data with mean values \n","train[names] = train[names].fillna(train[names].mean())\n","test[names] = test[names].fillna(train[names].mean())\n","# clip values to avoid different values in the test set from train\n","test = test[names].clip(train[names].min(axis=0).values,train[names].max(axis=0).values, axis=1)\n","\n","# data scaled to allow the features interaction (by multiplication)\n","scaler = StandardScaler()\n","\n","train2 = copy.copy(train)\n","teste2 = copy.copy(test)\n","\n","vals = scaler.fit_transform(train[names])\n","vals_test = scaler.transform(test[names])\n","\n","train2[names] = vals\n","teste2[names] = vals_test\n","\n","if CFG.nan_impute:\n","    train2['EL'] = train_df['EL_adj']\n","    teste2['EL'] = test_df['EL_adj']"]},{"cell_type":"markdown","metadata":{},"source":["# Defining 2 order interactions"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[],"source":["# def multiply and make a array of all interactions\n","def mab(df,nome1,nome2):\n","    a  = df[nome1]*df[nome2]\n","    return(a/max(a))\n","\n","h = []\n","ht = []\n","\n","n = 1\n","for n1 in names:\n","    for n2 in names[n:]:\n","        h.append(mab(train2,n1,n2).rename(n1+'_mul_'+n2))\n","        ht.append(mab(teste2,n1,n2).rename(n1+'_mul_'+n2))\n","        \n","    n+=1\n","    \n","newF = pd.DataFrame(h)\n","newF_test = pd.DataFrame(ht)"]},{"cell_type":"markdown","metadata":{},"source":["# Get IV and WOE features"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["#https://lucastiagooliveira.github.io/datascience/iv/woe/python/2020/12/15/iv_woe.html\n","def iv_woe(data, target, bins=10, show_woe=False):\n","    \n","    #Empty Dataframe\n","    newDF,woeDF = pd.DataFrame(), pd.DataFrame()\n","    \n","    #Extract Column Names\n","    cols = data.columns\n","    \n","    #Run WOE and IV on all the independent variables\n","    for ivars in cols[~cols.isin([target])]:\n","        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n","            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n","            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n","        else:\n","            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n","\n","        \n","        # Calculate the number of events in each group (bin)\n","        d = d0.groupby(\"x\", as_index=False).agg({\"y\": [\"count\", \"sum\"]})\n","        d.columns = ['Cutoff', 'N', 'Events']\n","        \n","        # Calculate % of events in each group.\n","        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()\n","\n","        # Calculate the non events in each group.\n","        d['Non-Events'] = d['N'] - d['Events']\n","        # Calculate % of non events in each group.\n","        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()\n","\n","        # Calculate WOE by taking natural log of division of % of non-events and % of events\n","        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])\n","        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])\n","        d.insert(loc=0, column='Variable', value=ivars)\n","        #print(\"Information value of \" + ivars + \" is \" + str(round(d['IV'].sum(),6)))\n","        temp =pd.DataFrame({\"Variable\" : [ivars], \"IV\" : [d['IV'].sum()]}, columns = [\"Variable\", \"IV\"])\n","        newDF=pd.concat([newDF,temp], axis=0)\n","        woeDF=pd.concat([woeDF,d], axis=0)\n","\n","        #Show WOE Table\n","        if show_woe == True:\n","            print(d)\n","    return newDF, woeDF\n","\n","a,b = iv_woe(train2, target_name, bins=10, show_woe=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array(['k', 'Id', 'DU', 'GL', 'FL', 'CR', 'DA', 'AF', 'AB', 'BQ', 'DI',\n","       'EB', 'FD', 'EE', 'EH', 'FR', 'CD', 'DE', 'CC', 'BN', 'FI', 'FE',\n","       'DH', 'EU', 'GF', 'DF', 'BC', 'DL', 'AM', 'BP', 'AH', 'AR', 'GH',\n","       'DN', 'CS', 'GB', 'DY', 'CF', 'CB', 'GI', 'BD', 'FC', 'BR', 'CU',\n","       'EL', 'FS', 'AZ', 'EJ', 'CW', 'AX', 'GE', 'AY', 'EG', 'EP', 'CH',\n","       'CL', 'BZ', 'DV'], dtype=object)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# most important features based on IV\n","a.sort_values(by='IV',ascending=False).Variable.values "]},{"cell_type":"markdown","metadata":{},"source":["# Prepare dataset with the new features"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["# Reordering the dataframe to keep IV with higger values in front\n","trainE = train[a.sort_values(by='IV',ascending=False).Variable.values]\n","trainE[target_name] = train[target_name]\n","testeE = test[a.sort_values(by='IV',ascending=False).Variable.values[2:]]\n","\n","# join the original vars and the interactions between them\n","ff = pd.concat([trainE,newF.T],axis=1)\n","ff_teste = pd.concat([testeE,newF_test.T],axis=1)\n","\n","a,b = iv_woe(ff, target_name, bins=10, show_woe=False)\n","\n","# deleting all IVs below 0.05\n","a = a.loc[a['IV']> 0.05]\n","\n","allNames = a.sort_values(by='IV',ascending=False).Variable.values\n","crossNames = [x for x in allNames if '_mul_' in x]\n","\n","nomes2 = list(trainE)+crossNames\n","nomes2.remove('Class')"]},{"cell_type":"markdown","metadata":{},"source":["# Set threshold for correlation features"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["threshold = 0.3\n","\n","cc = ff[nomes2[2:]].corr()\n","\n","mat_x = abs(cc)>threshold\n","mat_x = mat_x.to_numpy()"]},{"cell_type":"markdown","metadata":{},"source":["# Select variables with low correlation"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["# there are +- 70 features with low correlation\n","var1 = []\n","nomes = list(cc)\n","var1.append(nomes[0])\n","max_vars = 100\n","\n","count = 1\n","for n in range(1,len(nomes)):\n","    \n","    if (mat_x[n,:n+1].sum() ) == 1:\n","        \n","        var1.append(nomes[n])        \n","        count+=1\n","        \n","        if(count == max_vars):\n","            break"]},{"cell_type":"markdown","metadata":{},"source":["# Drop features that get low score"]},{"cell_type":"code","execution_count":20,"metadata":{"trusted":true},"outputs":[],"source":["# 'CW', 'AZ', 'FS', 'BR', 'FE', 'BN', 'DE', 'AF', 'CR',\n","\n","features_to_drop = ['CR_mul_DE', 'BQ_mul_FE', 'CR_mul_GE', 'EE_mul_GF', \n","                    'CR_mul_FE', 'BQ_mul_FC', 'DE_mul_DL', 'AZ_mul_GL', 'CW_mul_DL', \n","                    'BN_mul_CR', 'DN_mul_FI', 'AZ_mul_FE', 'CW_mul_EL', 'AZ_mul_CU',\n","                    'CW_mul_DY', 'DH_mul_DL', 'AX_mul_CU', 'BN_mul_DE', 'BN_mul_CW', \n","                    'AZ_mul_EL', 'AZ_mul_DE']\n","\n","var1 = [v for v in var1 if v not in features_to_drop]"]},{"cell_type":"markdown","metadata":{},"source":["# Create dict with WoE transformation"]},{"cell_type":"code","execution_count":21,"metadata":{"trusted":true},"outputs":[],"source":["# create dic with WoE transformation\n","list_dics = []\n","\n","for var in var1:\n","    df_temp = b.loc[b['Variable']==var].reset_index()\n","    # crieate dict\n","    dict_var = {}\n","    for x in range(len(df_temp)):\n","        line = df_temp.iloc[x]\n","        dict_var[line['Cutoff']] = line['WoE']\n","    list_dics.append(dict_var)"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare train and test data"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["# train and test data\n","df_original = ff[var1+[target_name] + ['k'] ]\n","df_test2 = ff_teste[var1]\n","names = var1"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[],"source":["# In this part there is some data leakage as the map is using the full dataset\n","n=0\n","\n","for var in var1:\n","    df_original.loc[:,var] = df_original[var].map(list_dics[n])\n","    df_test2.loc[:,var] = df_test2[var].map(list_dics[n])\n","    n = n + 1"]},{"cell_type":"markdown","metadata":{},"source":["# Fill NaNs"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["df_original.loc[:,names] = df_original[names].fillna(df_original[names].mean())\n","df_test2.loc[:,names] = df_test2[names].fillna(df_original[names].mean())"]},{"cell_type":"markdown","metadata":{},"source":["# Train LR"]},{"cell_type":"code","execution_count":25,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["0.18202025653081474"]},"metadata":{},"output_type":"display_data"}],"source":["n_splits = 10\n","\n","predictions_LR = 0\n","cv_score_LR = 0\n","\n","rr = [42, 21, 100, 45, 1, 228]\n","\n","oof_level2_LR = np.zeros([df_original['Class'].shape[0], len(rr)])\n","oof_level2_LR_test = np.zeros([test_df.shape[0], len(rr)])\n","\n","for f, v_fold in enumerate(rr):\n","    skf = StratifiedKFold(n_splits=CFG.n_stacking_folds, shuffle=True, random_state=3082023)\n","    for i, (train_index, val_index) in enumerate(skf.split(df_original[names], df_original['Class'])):\n","\n","            model = LogisticRegression(random_state=3082023+i, C=0.1, n_jobs=-1, max_iter=2000, class_weight='balanced')\n","\n","            df_train = df_original.iloc[train_index]\n","            df_val = df_original.iloc[val_index]\n","            \n","            df_train1 = df_train[names].to_numpy()\n","            df_val1 = df_val[names].to_numpy()            \n","            \n","            model.fit(df_train1, df_train['Class'])\n","            \n","            y_hat_val_LR = model.predict_proba(df_val1)[:,1]\n","            val = balanced_log_loss(df_val[target_name], y_hat_val_LR.reshape(-1, ))\n","            \n","            try:\n","                oof_level2_LR[val_index, f] = model.predict_proba(df_val1)[:,1]\n","                oof_level2_LR_test[:,f] = model.predict_proba(df_test2[names])[:,1]\n","            except:\n","                oof_level2_LR[val_index, f] = np.zeros(len(val_index))\n","                oof_level2_LR_test[:,f] = np.zeros(df_test2.shape[0])\n","\n","# Use Optuna to find the best ensemble weights\n","optweights = OptunaWeights(random_state=10082023)\n","y_val_pred = optweights.fit_predict(y, [oof_level2_LR[:,i] for i in range(oof_level2_LR.shape[1])])\n","optuna_weights_LR = np.array(optweights.weights)\n","display(balanced_log_loss(y, y_val_pred))\n","\n","# oof_level2_LR = (optuna_weights_LR * oof_level2_LR).sum(axis=1) / sum(optuna_weights_LR)\n","# oof_level2_LR_test = (optuna_weights_LR * oof_level2_LR_test).sum(axis=1) / sum(optuna_weights_LR)\n","\n","oof_level2_LR = oof_level2_LR.mean(axis=1)\n","oof_level2_LR_test = oof_level2_LR_test.mean(axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["# ======================================================\n","# Get ensemble predictions"]},{"cell_type":"code","execution_count":26,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([0.63276243, 0.34647374])"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["0.15979566349415253"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["oof_level2 = [oof_level2_LGBM, oof_level2_LR]\n","oof_level2_test = [oof_level2_LGBM_test, oof_level2_LR_test]\n","\n","# Use Optuna to find the best ensemble weights\n","optweights = OptunaWeights(random_state=19072023)\n","y_val_pred = optweights.fit_predict(y, [oof_level2[i] for i in range(len(oof_level2))])\n","optuna_weights = np.array(optweights.weights)\n","# optuna_weights[optuna_weights < 0.05] = 0\n","display(optuna_weights)\n","balanced_log_loss(y, y_val_pred)"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[],"source":["# array([0.9439178 , 0.88226912])\n","\n","# 0.15875810163589982"]},{"cell_type":"markdown","metadata":{},"source":["# Predict test"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>class_0</th>\n","      <th>class_1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00eed32682bb</td>\n","      <td>0.467</td>\n","      <td>0.533</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>010ebe33f668</td>\n","      <td>0.467</td>\n","      <td>0.533</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>02fa521e1838</td>\n","      <td>0.467</td>\n","      <td>0.533</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>040e15f562a2</td>\n","      <td>0.467</td>\n","      <td>0.533</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>046e85c7cc7f</td>\n","      <td>0.467</td>\n","      <td>0.533</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             Id  class_0  class_1\n","0  00eed32682bb    0.467    0.533\n","1  010ebe33f668    0.467    0.533\n","2  02fa521e1838    0.467    0.533\n","3  040e15f562a2    0.467    0.533\n","4  046e85c7cc7f    0.467    0.533"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["def predict(X):\n","    y = np.zeros_like(X[0])\n","    for i in range(len(X)):\n","        # y += oof_level2_test[i]\n","        y += optuna_weights[i] * X[i]\n","    # return y / len(X)\n","    return y / sum(optuna_weights)\n","\n","def lr_predict(X):\n","    return lr.predict_proba(X)[:,1]\n","\n","predictions = predict(oof_level2_test)\n","# predictions = lr_predict(oof_level2_test)\n","\n","if CFG.adjust_class_threshold:\n","    _, predictions = inflate_preds(y, np.mean(X, axis=1), predictions)\n","\n","predictions = np.nan_to_num(predictions)\n","test_df['class_1'] = np.round(predictions, 15)\n","test_df['class_0'] = 1 - predictions\n","\n","sample_submission[['class_0', 'class_1']] = test_df[['class_0', 'class_1']]\n","sample_submission.to_csv(r\"submission.csv\", index=False)\n","sample_submission"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
